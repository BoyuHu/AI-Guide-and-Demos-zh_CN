{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347c39eb-0974-4880-b6dd-0cbfc33d7145",
   "metadata": {},
   "source": [
    "# 简介\n",
    "\n",
    "这里是文章[14. PEFT：在大模型中快速应用 LoRA]() 所涉及的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c339a9-af32-494f-88f0-1a5859da483d",
   "metadata": {},
   "source": [
    "## 安装必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c27f94-265c-49f3-a624-7f089f426577",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006b231-0d05-444a-bdf8-4dcfed09f1c0",
   "metadata": {},
   "source": [
    "## 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8899fbab-45d1-43be-b198-403903d36900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 加载预训练的 GPT-2 模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c18030-1a96-4ba9-a157-bc4bf9293352",
   "metadata": {},
   "source": [
    "## 使用 PEFT 应用 LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd0f0a4-f692-47c1-b153-3eb5f38aded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 配置 LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # 任务类型：因果语言模型\n",
    "    inference_mode=False,          # 推理模式关闭，以进行训练\n",
    "    r=8,                           # 低秩值 r\n",
    "    lora_alpha=32,                 # LoRA 的缩放因子\n",
    "    lora_dropout=0.1,              # Dropout 概率\n",
    ")\n",
    "\n",
    "# 将 LoRA 应用到模型中\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658099e-7627-4950-b9ce-31167a9f6b3c",
   "metadata": {},
   "source": [
    "## 查看当前模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b200673-a8e1-42bf-ad23-a6944ee3f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D()\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a89d20-f03d-4938-94c8-42c79dae4fc8",
   "metadata": {},
   "source": [
    "## 查看增加的参数量\n",
    "\n",
    "应用 LoRA 后，我们一般都希望了解模型参数量的变化，它的计算其实很简单，\n",
    "\n",
    "### 理论计算\n",
    "\n",
    "对于每个应用了 LoRA 的层，增加的参数量为：\n",
    "\n",
    "$$\n",
    "\\text{增加的参数量} = r \\times (\\text{输入维度} + \\text{输出维度})\n",
    "$$\n",
    "\n",
    "- **`r`**：LoRA 的低秩值。\n",
    "- **输入维度**：层的输入特征数。\n",
    "- **输出维度**：层的输出特征数。\n",
    "\n",
    "### 使用 PEFT 查看参数\n",
    "\n",
    "`peft` 提供了查看模型参数的便捷方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db8ddcf3-1ea0-4bfc-989e-890608e65e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364\n"
     ]
    }
   ],
   "source": [
    "# 查看 LoRA 模块\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f2a584-f2f3-49c3-a010-0dbe3962fe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可训练参数量: 294912\n",
      "总参数量: 124734720\n",
      "可训练参数占比: 0.24%\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        all_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    print(f\"可训练参数量: {trainable_params}\")\n",
    "    print(f\"总参数量: {all_params}\")\n",
    "    print(f\"可训练参数占比: {100 * trainable_params / all_params:.2f}%\")\n",
    "    \n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe2425-7bf6-4fb3-be8e-a7f66257828e",
   "metadata": {},
   "source": [
    "## 准备数据并进行微调\n",
    "\n",
    "假设你已经有了训练数据集 `train_dataset`，下面是一个简单的样例代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289cbbe-db23-4858-b58f-7b937e93afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',         # 模型保存和日志输出的目录路径\n",
    "    num_train_epochs=3,             # 训练的总轮数（epochs）\n",
    "    per_device_train_batch_size=16, # 每个设备（如GPU或CPU）上的训练批次大小，16表示每次输入模型的数据数量\n",
    "    learning_rate=5e-5,             # 学习率\n",
    "    logging_steps=10,               # 每隔多少步（steps）进行一次日志记录\n",
    "    save_steps=100,                 # 每隔多少步保存模型\n",
    ")\n",
    "\n",
    "# 创建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                    # 训练的模型对象，需要事先加载好\n",
    "    args=training_args,             # 上面定义的训练参数配置\n",
    "    train_dataset=train_dataset,    # 需要对应替换成已经处理过的dataset\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70f7e3-a20b-4e01-8931-cfcc1e655cb1",
   "metadata": {},
   "source": [
    "### 保存和加载 LoRA 微调的模型\n",
    "\n",
    "训练完成后，你可以保存或者加载 LoRA 微调的参数，下面是个简单的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e02e8-d3ab-457a-b8cc-0eb8ec715bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 LoRA 参数\n",
    "model.save_pretrained('./lora_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9771b8-de83-440c-9fa7-b1f1506ed64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载原始模型\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 加载 LoRA 参数\n",
    "from peft import PeftModel\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, './lora_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
