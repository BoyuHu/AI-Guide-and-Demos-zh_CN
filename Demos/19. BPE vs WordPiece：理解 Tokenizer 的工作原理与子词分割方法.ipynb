{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d293fb3-7b27-4c56-ad0e-1d681360265f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# BPE vs WordPiece：理解 Tokenizer 的工作原理与子词分割方法\n",
    "\n",
    "> 当前代码文件完全镜像了[文章 21](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/21.%20BPE%20vs%20WordPiece：理解%20Tokenizer%20的工作原理与子词分割方法.md) 的内容，可以仅阅读该文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0eb806-1599-4d18-9bf5-ed75c7220bd8",
   "metadata": {},
   "source": [
    "> 在应用的路上“蒙着头”走了一段，是时候回过头来理解其中的工作原理了。\n",
    ">\n",
    "> 文章将以文本处理为例，介绍数据预处理中的关键组件——**Tokenizer（分词器）**。需要注意的是，这里是偏概念性的讲解，不会严谨地讨论具体函数的参数细节。\n",
    ">\n",
    "> 「构造词汇表」部分将介绍两种常见的子词分割方法：\n",
    ">\n",
    "> - **BPE（Byte-Pair Encoding）**：用于 GPT、GPT-2、RoBERTa、BART 和 DeBERTa 等模型。\n",
    "> - **WordPiece**：用于 DistilBERT、MobileBERT、Funnel Transformers 和 MPNET 等模型。\n",
    ">\n",
    "> 工具：[Tiktokenizer（推荐）](https://tiktokenizer.vercel.app) | [The Tokenizer Playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)\n",
    "\n",
    "\n",
    "## 什么是 Tokenizer？\n",
    "\n",
    "**Tokenizer**（分词器）可以将原始文本（raw text）转换为模型能够理解的数字序列，在模型输入和输出的两个主要阶段中发挥重要作用：\n",
    "\n",
    "### 模型输入（编码 Encode）阶段\n",
    "\n",
    "1. **分词（Tokenize）**\n",
    "\n",
    "   将文本拆分为词元（Token），常见的分词方式包括字级、词级、子词级（如 BPE、WordPiece）、空格分词等。\n",
    "\n",
    "   ```sql\n",
    "   输入: \"你好\"\n",
    "   分词: [\"你\", \"好\"]\n",
    "   ```\n",
    "\n",
    "2. **映射（Mapping）**\n",
    "\n",
    "   将每个词元映射为词汇表中的唯一 ID，生成的数字序列即为模型的输入。\n",
    "\n",
    "   ```sql\n",
    "   分词: [\"你\", \"好\"]\n",
    "   映射: [1001, 1002]\n",
    "   ```\n",
    "\n",
    "### 模型输出（解码 Decode）阶段\n",
    "\n",
    "1. **反映射（De-mapping）**\n",
    "\n",
    "   模型输出的数字序列通过词汇表映射回对应的词元，二者是一一对应的关系。\n",
    "\n",
    "   ```sql\n",
    "   输出: [1001, 1002]\n",
    "   反映射: [\"你\", \"好\"]\n",
    "   ```\n",
    "\n",
    "2. **文本重组**\n",
    "\n",
    "   将解码后的词元以某种规则重新拼接为完整文本。\n",
    "\n",
    "   ```sql\n",
    "   反映射: [\"你\", \"好\"]\n",
    "   重组: \"你好\"\n",
    "   ```\n",
    "\n",
    "### 直观感受\n",
    "\n",
    "访问 [Tiktokenizer](https://tiktokenizer.vercel.app)，通过右上角选取不同的 Tokenizer 进行尝试：\n",
    "\n",
    "![image-20241022152315606](../Guide/assets/image-20241022152315606.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b3a399-48ba-4f1f-aa7b-68e69a9b4e74",
   "metadata": {},
   "source": [
    "## 实际使用\n",
    "在进一步讲解之前，我们先通过 **Transformers** 库中的 `AutoTokenizer` 类来使用 Tokenizer。\n",
    "\n",
    "### 安装库\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf279482-ba17-48ef-9aaf-cd447e6249b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d1a72-a07e-4394-a448-746e74b0e4b0",
   "metadata": {},
   "source": [
    "### BPE 分词器示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81d62a7-97cc-43a4-8df2-2827fa3cfc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello', ',', 'Ġworld', '!']\n",
      "Token IDs: [15496, 11, 995, 0]\n",
      "Tokens: ['Hello', ',', 'Ġworld', '!']\n",
      "Decoded Text: Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 使用 GPT-2 的分词器（BPE）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "# 编码\n",
    "# 1. 将文本分词为 Tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. 将 Tokens 转换为 Token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# 解码\n",
    "# 1. Token IDs 转换为 Tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. Tokens 拼接为文本\n",
    "decoded_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86bd1e-875c-4949-8a35-547e85b02951",
   "metadata": {},
   "source": [
    "### WordPiece 分词器示例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfaafd8-12e2-48b7-8a60-c3e53d4d48f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', ',', 'world', '!']\n",
      "Token IDs: [7592, 1010, 2088, 999]\n",
      "Tokens: ['hello', ',', 'world', '!']\n",
      "Decoded Text: hello, world!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 使用 BERT 的分词器（WordPiece）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "# 编码\n",
    "# 1. 将文本分词为 Tokens\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. 将 Tokens 转换为 Token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# 解码\n",
    "# 1. Token IDs 转换为 Tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 2. Tokens 拼接为文本\n",
    "decoded_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9ae48-58e4-4e96-b7b4-947f00ec146a",
   "metadata": {},
   "source": [
    "### 使用 `encode()` 和 `decode()` 方法\n",
    "\n",
    "更简洁且常见的使用方式是直接使用 `encode()` 和 `decode()` 方法：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56cab755-c719-4742-a5d4-35e5dc4a7cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [15496, 11, 995, 0]\n",
      "Decoded Text: Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 取消注释以对比两种分词器的输出差异\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "# 使用 encode() 将文本直接转换为 Token IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# 使用 decode() 将 Token IDs 转换回文本\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c9cd4-66cd-4601-b938-e48cc814d5f9",
   "metadata": {},
   "source": [
    "## 了解 Tokenizer 的基础属性\n",
    "\n",
    "导入分词器后，可以选择查看一些属性来获得直观的理解，例如查看词汇表、特殊标记等，以 GPT-2 为例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9345315-83db-484a-9945-360d26f8ae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 50257\n"
     ]
    }
   ],
   "source": [
    "# 获取词汇表大小\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107665fe-51d3-4f65-8f5a-1aa3e4aeace5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ġcopyrighted': 33696,\n",
       " 'agar': 32452,\n",
       " 'ĠFeel': 18571,\n",
       " 'ĠYor': 42453,\n",
       " 'Ġgenerally': 4143,\n",
       " 'Ġinstances': 10245,\n",
       " 'Ġthumbnail': 40901,\n",
       " 'aste': 4594,\n",
       " 'ĠDebate': 41029,\n",
       " 'wrapper': 48553,\n",
       " 'Ġseizure': 22338,\n",
       " 'Ġhumans': 5384,\n",
       " 'Ġturf': 29553,\n",
       " 'Cond': 25559,\n",
       " 'ĠSpo': 49331,\n",
       " 'The': 464,\n",
       " 'Ġpatch': 8529,\n",
       " 'ĠCy': 5934,\n",
       " 'ĠUntil': 14303,\n",
       " 'ogene': 20878,\n",
       " 'Ġrespect': 2461,\n",
       " 'FILE': 25664,\n",
       " 'Rel': 6892,\n",
       " 'eton': 18483,\n",
       " 'ĠError': 13047,\n",
       " 'Ġslope': 22638,\n",
       " 'cop': 22163,\n",
       " 'ĠClouds': 46298,\n",
       " 'mor': 4491,\n",
       " 'ĠGates': 15953,\n",
       " 'ĠGreat': 3878,\n",
       " 'Ġlifelong': 25837,\n",
       " 'ĠSask': 23541,\n",
       " 'Ġclan': 19065,\n",
       " 'iggins': 23567,\n",
       " 'ĠTECH': 44999,\n",
       " 'ĠAmbassador': 20656,\n",
       " 'ĠRand': 8790,\n",
       " 'Ġencompasses': 38932,\n",
       " 'ĠSDK': 26144,\n",
       " 'Ġfid': 49909,\n",
       " 'Ġgallon': 26860,\n",
       " 'Ġbrawl': 42599,\n",
       " 'Ġscathing': 44523,\n",
       " 'Ġseemed': 3947,\n",
       " 'erm': 7780,\n",
       " 'ĠPact': 41460,\n",
       " 'Ġneocons': 43530,\n",
       " 'uck': 1347,\n",
       " 'Ġcrash': 7014,\n",
       " 'eco': 47704,\n",
       " 'ĠWIN': 25779,\n",
       " 'ĠWeb': 5313,\n",
       " 'ĠRhodes': 25656,\n",
       " 'anking': 15230,\n",
       " '269': 26276,\n",
       " 'Transfer': 43260,\n",
       " 'Ġsp': 599,\n",
       " 'Ġphotos': 5205,\n",
       " 'ye': 5948,\n",
       " 'yg': 35641,\n",
       " 'ĠPace': 44111,\n",
       " 'Ġclues': 20195,\n",
       " 'yx': 28391,\n",
       " 'Ġdocumentaries': 43014,\n",
       " 'Ġunanim': 17452,\n",
       " 'ĠBarack': 8732,\n",
       " 'atre': 10562,\n",
       " 'Ġlamps': 32209,\n",
       " 'ĠKan': 14248,\n",
       " 'Mayor': 37396,\n",
       " 'Ġslice': 16416,\n",
       " 'ĠSob': 36884,\n",
       " 'Ġdiminishing': 35197,\n",
       " 'who': 8727,\n",
       " 'ĠAzure': 22134,\n",
       " 'igs': 9235,\n",
       " 'ĠContent': 14041,\n",
       " 'ĠPrinceton': 23173,\n",
       " 'Ġcalculates': 43707,\n",
       " 'Ġclamp': 29405,\n",
       " 'chel': 29232,\n",
       " 'Russell': 46325,\n",
       " 'Ġdictator': 26671,\n",
       " 'velop': 1091,\n",
       " 'av': 615,\n",
       " 'Ġcue': 28381,\n",
       " 'ĠMinisters': 36647,\n",
       " 'ohm': 34028,\n",
       " 'ĠBoise': 38830,\n",
       " 'Ġdrama': 10512,\n",
       " 'edited': 42131,\n",
       " 'ĠRevenue': 20197,\n",
       " 'Hor': 27991,\n",
       " 'bral': 24427,\n",
       " 'XT': 25010,\n",
       " 'YY': 26314,\n",
       " 'Short': 16438,\n",
       " 'ackers': 28874,\n",
       " 'ĠBO': 16494,\n",
       " 'Ġdirectly': 3264,\n",
       " 'Ġdissect': 38319,\n",
       " 'Ġanatomy': 33449,\n",
       " 'ĠWire': 14712,\n",
       " 'sych': 2924,\n",
       " 'Ġdocument': 3188,\n",
       " 'Ġevolution': 6954,\n",
       " '163': 24136,\n",
       " 'Ġhashes': 46621,\n",
       " 'Ġlewd': 45179,\n",
       " 'zh': 23548,\n",
       " 'bottom': 22487,\n",
       " 'Values': 40161,\n",
       " 'B': 33,\n",
       " '´': 112,\n",
       " 'ĠPsychiatry': 28897,\n",
       " 'marg': 30887,\n",
       " 'ĠBuddhism': 25932,\n",
       " 'Ġexecutes': 42985,\n",
       " 'oyle': 19802,\n",
       " 'ĠInjury': 40883,\n",
       " 'Ġelev': 7662,\n",
       " 'Ġlimbo': 46008,\n",
       " 'inski': 21141,\n",
       " 'ĠFiesta': 49839,\n",
       " 'Ġobservation': 13432,\n",
       " 'KY': 31159,\n",
       " 'large': 11664,\n",
       " 'Ġpostings': 44656,\n",
       " 'Ġpsychedelic': 34490,\n",
       " 'Ġrail': 6787,\n",
       " 'Ġanarchists': 29676,\n",
       " 'Ġanarchist': 26177,\n",
       " 'Ġreassuring': 36450,\n",
       " 'Ġclashes': 21022,\n",
       " 'ĠNieto': 49783,\n",
       " 'Ġclueless': 44865,\n",
       " 'Ġdisclose': 15771,\n",
       " 'ĠYun': 20757,\n",
       " 'Ġtactics': 10815,\n",
       " 'Ġtexts': 13399,\n",
       " 'Ġtwelve': 14104,\n",
       " 'Ġlodged': 31984,\n",
       " 'Ġresisted': 26643,\n",
       " 'Ġelectrom': 26629,\n",
       " 'apixel': 48633,\n",
       " 'Ġ93': 10261,\n",
       " 'Ġovertly': 45118,\n",
       " 'Ġmini': 9927,\n",
       " 'Ġrapes': 37459,\n",
       " 'Ġfactor': 5766,\n",
       " 'achev': 42961,\n",
       " 'ĠBloomberg': 15689,\n",
       " 'Ġballpark': 40598,\n",
       " 'Ġmediation': 41077,\n",
       " 'Ġsett': 2970,\n",
       " 'ĠEditorial': 39525,\n",
       " 'Ġsoldier': 10686,\n",
       " 'Ġwalked': 6807,\n",
       " 'atherine': 15289,\n",
       " 'clus': 2527,\n",
       " 'Ġtiles': 19867,\n",
       " 'dirty': 49075,\n",
       " 'Ġappre': 5208,\n",
       " 'mid': 13602,\n",
       " 'ĠCategories': 45486,\n",
       " 'Ġbusiest': 42745,\n",
       " 'rowth': 13046,\n",
       " 'erg': 6422,\n",
       " 'Ġbeet': 25679,\n",
       " 'ĠKenya': 21506,\n",
       " 'Ġchants': 40727,\n",
       " 'POR': 44680,\n",
       " 'uu': 12303,\n",
       " 'Ġphones': 9512,\n",
       " 'ĠDisabled': 43201,\n",
       " 'Ġ227': 30989,\n",
       " 'Ġpoet': 21810,\n",
       " 'ĠComet': 36238,\n",
       " 'Ġsil': 3313,\n",
       " 'ly': 306,\n",
       " 'ensitivity': 40545,\n",
       " 'Ġcontribution': 10156,\n",
       " 'ĠAmelia': 49769,\n",
       " 'ĠBreakfast': 32175,\n",
       " 'Gamer': 33648,\n",
       " 'ĠStability': 47865,\n",
       " 'ophysical': 41789,\n",
       " 'having': 40965,\n",
       " 'Ġremoval': 9934,\n",
       " 'Ġmoney': 1637,\n",
       " 'ĠHDL': 48867,\n",
       " 'Active': 13739,\n",
       " 'ĠFellow': 29764,\n",
       " 'ĠHerb': 34858,\n",
       " 'ĠAlzheimer': 22434,\n",
       " 'ĠNG': 39058,\n",
       " 'Ġvomit': 44542,\n",
       " 'emphasis': 36663,\n",
       " 'Social': 20636,\n",
       " 'ĠBihar': 45301,\n",
       " 'ĠCursed': 29492,\n",
       " 'Ġintent': 6824,\n",
       " 'Ġdistances': 18868,\n",
       " 'Ġsans': 38078,\n",
       " 'def': 4299,\n",
       " 'ĠQuantum': 29082,\n",
       " '649': 33300,\n",
       " 'Winter': 35376,\n",
       " 'Ġaccording': 1864,\n",
       " 'Ġactually': 1682,\n",
       " 'ĠEVEN': 45886,\n",
       " 'ĠUganda': 30872,\n",
       " 'Ġaft': 46088,\n",
       " 'Ġearns': 29339,\n",
       " 'WARD': 39743,\n",
       " 'Ġhunts': 42984,\n",
       " 'Ġskillet': 41306,\n",
       " 'ĠExpend': 48623,\n",
       " 'Ġenthusiasm': 17131,\n",
       " 'ĠRohing': 32571,\n",
       " 'ĠMaduro': 33007,\n",
       " 'CRIPTION': 40165,\n",
       " 'Gameplay': 43241,\n",
       " 'ĠCrypto': 36579,\n",
       " 'Ġinsiders': 31594,\n",
       " 'ared': 1144,\n",
       " 'Ġlarvae': 37346,\n",
       " 'ZZ': 30148,\n",
       " 'Fail': 39044,\n",
       " 'loo': 29680,\n",
       " 'ĠDefinitive': 46634,\n",
       " 'Ġpipeline': 11523,\n",
       " 'Ġintegration': 11812,\n",
       " 'ĠAgents': 28295,\n",
       " 'SIGN': 46224,\n",
       " 'scribed': 47495,\n",
       " 'Visit': 31141,\n",
       " 'rolled': 8375,\n",
       " 'Ġne': 497,\n",
       " 'Ġpret': 2181,\n",
       " 'Isa': 39443,\n",
       " 'Ġbehind': 2157,\n",
       " 'Ġanxious': 18116,\n",
       " 'Plot': 43328,\n",
       " 'Ġsurvive': 7866,\n",
       " 'ĠSeptember': 2693,\n",
       " 'Ġvi': 25357,\n",
       " 'iasco': 40025,\n",
       " 'Ġillustrious': 47887,\n",
       " 'ĠEchoes': 45547,\n",
       " 'ĠPatron': 43315,\n",
       " 'ĠAirbus': 39173,\n",
       " 'Ġhomage': 31044,\n",
       " 'hai': 44488,\n",
       " 'clone': 21018,\n",
       " 'Ġaltitude': 20334,\n",
       " 'Ġcups': 14180,\n",
       " 'Ġlikelihood': 14955,\n",
       " 'Ġmascara': 48430,\n",
       " 'ĠLSU': 28078,\n",
       " 'Ġrelocation': 35703,\n",
       " 'centered': 38050,\n",
       " 'Ġloves': 10408,\n",
       " 'ĠPenalty': 41676,\n",
       " 'Ġpresentations': 27709,\n",
       " 'break': 9032,\n",
       " 'ĠNag': 15196,\n",
       " 'ĠWouldn': 43048,\n",
       " 'ĠBloom': 11891,\n",
       " 'Ġexamine': 10716,\n",
       " 'olulu': 39814,\n",
       " 'Ġreign': 13580,\n",
       " 'Ġnull': 9242,\n",
       " 'NAS': 18293,\n",
       " 'adra': 49456,\n",
       " 'awaited': 41742,\n",
       " '324': 33916,\n",
       " 'âĢ¦âĢ¦âĢ¦âĢ¦': 15864,\n",
       " 'ĠBuilder': 35869,\n",
       " 'flight': 22560,\n",
       " 'ĠCorinthians': 39546,\n",
       " 'ĠDave': 9935,\n",
       " 'asion': 4247,\n",
       " 'ĠRequirements': 24422,\n",
       " 'Ġassembling': 40525,\n",
       " 'ific': 811,\n",
       " 'ĠMU': 25108,\n",
       " 'ĠPhilippe': 39393,\n",
       " 'Ġbadge': 23009,\n",
       " 'ĠMagnus': 35451,\n",
       " 'Ġheight': 6001,\n",
       " 'Ġrectangle': 35991,\n",
       " 'Ġ02': 7816,\n",
       " 'olph': 10196,\n",
       " 'pos': 1930,\n",
       " '156': 21599,\n",
       " 'Ġ302': 32591,\n",
       " 'ĠReneg': 41261,\n",
       " 'ĠValhalla': 48762,\n",
       " 'Met': 9171,\n",
       " 'uploads': 39920,\n",
       " 'ĠTone': 45362,\n",
       " 'Certainly': 36001,\n",
       " 'imov': 44273,\n",
       " 'tab': 8658,\n",
       " 'ĠDX': 19393,\n",
       " 'Make': 12050,\n",
       " 'ĠThieves': 40392,\n",
       " 'Ġdying': 9950,\n",
       " 'Ġheated': 16968,\n",
       " 'cup': 25244,\n",
       " 'Ġdrain': 14782,\n",
       " 'Ġimprovement': 9025,\n",
       " 'foo': 21943,\n",
       " 'Ġwonders': 19294,\n",
       " 'Ġpotions': 28074,\n",
       " 'Ġfixed': 5969,\n",
       " 'Ġcut': 2005,\n",
       " 'ĠFinals': 19705,\n",
       " 'uben': 44636,\n",
       " 'Ġinternship': 42329,\n",
       " 'Ġsmokes': 44589,\n",
       " 'oodle': 27106,\n",
       " 'Ġtut': 9732,\n",
       " 'Ġmanifested': 34300,\n",
       " 'using': 3500,\n",
       " 'ctuary': 15258,\n",
       " 'WER': 45532,\n",
       " 'ĠOutlook': 30096,\n",
       " 'Ġprevailing': 26602,\n",
       " 'oys': 19417,\n",
       " 'ĠContinuous': 45012,\n",
       " 'idy': 19325,\n",
       " 'sectional': 44330,\n",
       " 'Ġalgorithm': 11862,\n",
       " 'ĠMillennium': 26139,\n",
       " 'Ġjumping': 14284,\n",
       " 'Ġobedient': 41652,\n",
       " 'arte': 32074,\n",
       " 'ĠRF': 20445,\n",
       " '19': 1129,\n",
       " 'escap': 50141,\n",
       " 'udeau': 16229,\n",
       " 'MS': 5653,\n",
       " 'ĠBerlin': 11307,\n",
       " 'Ġ({': 37913,\n",
       " 'Ġmilliseconds': 38694,\n",
       " 'Network': 26245,\n",
       " 'Ġtwo': 734,\n",
       " 'called': 7174,\n",
       " 'Ġquestion': 1808,\n",
       " 'Ġyawn': 40908,\n",
       " 'Ġbacked': 9763,\n",
       " 'eli': 43733,\n",
       " 'iral': 21093,\n",
       " 'ĠDing': 46980,\n",
       " 'eways': 43613,\n",
       " 'Ġinnovate': 47916,\n",
       " 'Ġpo': 745,\n",
       " '797': 44673,\n",
       " 'ĠMidlands': 47989,\n",
       " 'ãĤ¨ãĥ«': 46948,\n",
       " 'nuts': 31381,\n",
       " 'Ġsincerely': 29093,\n",
       " 'Ġexhilar': 47029,\n",
       " 'between': 23395,\n",
       " 'Ġ530': 40585,\n",
       " 'TERN': 31800,\n",
       " 'aques': 46806,\n",
       " 'ĠLun': 19948,\n",
       " 'ĠMPs': 14952,\n",
       " 'ivity': 3458,\n",
       " 'ĠRyder': 40238,\n",
       " 'ĠLOL': 35513,\n",
       " 'ĠSOS': 42707,\n",
       " 'Ġairlines': 26225,\n",
       " 'Ġbelonged': 19611,\n",
       " 'Ġdistinguishing': 38607,\n",
       " 'Ġinterviewer': 39877,\n",
       " 'enger': 6540,\n",
       " '477': 32883,\n",
       " 'Ġsa': 473,\n",
       " 'ĠPyro': 44954,\n",
       " 'ĠCort': 18418,\n",
       " 'Ġspeedy': 35564,\n",
       " 'Char': 12441,\n",
       " 'ĠHills': 14379,\n",
       " 'ooming': 30602,\n",
       " 'personal': 22682,\n",
       " 'Ġshed': 14999,\n",
       " 'Winged': 47418,\n",
       " 'ĠAthlet': 18093,\n",
       " 'herry': 13372,\n",
       " 'æ': 162,\n",
       " 'Ġinvolving': 7411,\n",
       " 'ĠEdmund': 35646,\n",
       " 'ĠPerfect': 16374,\n",
       " 'weak': 38695,\n",
       " 'Ġbranching': 49526,\n",
       " 'ction': 596,\n",
       " 'ĠGiant': 17384,\n",
       " 'esthesia': 34811,\n",
       " 'Ġemitted': 31234,\n",
       " 'Ġvegetation': 28459,\n",
       " 'ĠOy': 39447,\n",
       " '90': 3829,\n",
       " 'Eventually': 28724,\n",
       " 'broken': 25826,\n",
       " 'ĠYale': 19681,\n",
       " 'cod': 19815,\n",
       " 'angles': 27787,\n",
       " 'Ġgrade': 9559,\n",
       " 'Ġhalves': 37192,\n",
       " 'Ġmont': 40689,\n",
       " 'ĠApart': 22596,\n",
       " 'AIDS': 39338,\n",
       " 'antly': 3875,\n",
       " 'ĠMVP': 12742,\n",
       " 'Ġmethod': 2446,\n",
       " 'Ġdrinking': 7722,\n",
       " 'bees': 41712,\n",
       " 'ifies': 6945,\n",
       " 'Ġ=': 796,\n",
       " 'Ġbold': 10758,\n",
       " 'Seven': 31334,\n",
       " 'ocating': 27123,\n",
       " 'Ġrobot': 9379,\n",
       " 'athom': 32910,\n",
       " 'Catholic': 48919,\n",
       " 'ĠPalmer': 18918,\n",
       " 'Ġimpair': 17253,\n",
       " 'absor': 46303,\n",
       " 'ACY': 43300,\n",
       " 'ĠLikely': 45974,\n",
       " 'Ġ165': 21409,\n",
       " 'Ġjurors': 28792,\n",
       " 'binary': 39491,\n",
       " 'ute': 1133,\n",
       " 'Ġconjecture': 46768,\n",
       " 'Ġcreator': 13172,\n",
       " 'deals': 14302,\n",
       " 'ĠRuth': 22132,\n",
       " 'List': 8053,\n",
       " 'ach': 620,\n",
       " 'ENN': 34571,\n",
       " 'home': 11195,\n",
       " 'CC': 4093,\n",
       " 'ru': 622,\n",
       " 'ayne': 43906,\n",
       " 'Ġ295': 34772,\n",
       " 'oki': 18228,\n",
       " 'ĠTommy': 19919,\n",
       " 'encers': 42288,\n",
       " 'Ġendorsement': 17819,\n",
       " 'ropolis': 25986,\n",
       " 'ĠDexter': 26284,\n",
       " 'eln': 45542,\n",
       " 'æ°': 36365,\n",
       " 'ĠINV': 34899,\n",
       " 'ĠKoran': 44097,\n",
       " 'ĠPsycho': 38955,\n",
       " 'framework': 30604,\n",
       " 'Ġintense': 8157,\n",
       " 'Ġintention': 6778,\n",
       " 'Clark': 43250,\n",
       " 'Ġnowadays': 26760,\n",
       " 'ĠBloody': 30893,\n",
       " 'compl': 23855,\n",
       " 'ohn': 1562,\n",
       " 'Ġoffensively': 41774,\n",
       " 'Ġrouters': 41144,\n",
       " 'Ġsons': 11989,\n",
       " 'ĠJinping': 39823,\n",
       " 'Software': 25423,\n",
       " 'Ġbeast': 13824,\n",
       " 'ĠRequired': 20906,\n",
       " 'Baby': 36534,\n",
       " 'Plug': 23257,\n",
       " 'isher': 4828,\n",
       " 'âĹ': 15926,\n",
       " 'ĠRocket': 16920,\n",
       " 'ĠSolid': 15831,\n",
       " 'Ġimpetus': 43007,\n",
       " 'Ġreforming': 40446,\n",
       " 'Red': 7738,\n",
       " 'Ġuncon': 21254,\n",
       " 'Ġweeds': 36708,\n",
       " 'ld': 335,\n",
       " 'Ġsalient': 49156,\n",
       " 'ĠSpeaker': 14931,\n",
       " 'Ġadministration': 3662,\n",
       " 'Any': 7149,\n",
       " 'Bul': 33481,\n",
       " 'butt': 43059,\n",
       " 'Ġguarantees': 19026,\n",
       " 'Ġbackdrop': 26373,\n",
       " 'Ġflickering': 50104,\n",
       " 'r': 81,\n",
       " 'Ġmechanisms': 11701,\n",
       " 'ĳå£«': 39374,\n",
       " 'Ġprincess': 21752,\n",
       " 'IGH': 18060,\n",
       " 'ĠDavies': 26618,\n",
       " 'Ġoffences': 20819,\n",
       " 'Fer': 43362,\n",
       " 'Vs': 23266,\n",
       " '34': 2682,\n",
       " 'ieu': 22304,\n",
       " 'ĠChinatown': 47043,\n",
       " 'ĠDisneyland': 36312,\n",
       " '.': 13,\n",
       " 'ITED': 22061,\n",
       " 'Ġonslaught': 36847,\n",
       " 'legged': 40898,\n",
       " 'Ġpro': 386,\n",
       " 'Ġtoxin': 43026,\n",
       " 'ushes': 17237,\n",
       " 'soDeliveryDate': 39811,\n",
       " 'ĠCoat': 39326,\n",
       " 'Ġdatas': 19395,\n",
       " 'FOX': 47853,\n",
       " 'Ġworld': 995,\n",
       " 'ĠPse': 49693,\n",
       " 'Ġvoltage': 15004,\n",
       " 'Ġbisexual': 24249,\n",
       " 'Ġsafe': 3338,\n",
       " 'north': 43588,\n",
       " 'Ġcensorship': 21185,\n",
       " 'ĠAsuka': 46649,\n",
       " 'inctions': 31253,\n",
       " 'ĠSinai': 33745,\n",
       " 'left': 9464,\n",
       " 'small': 17470,\n",
       " 'XXX': 43145,\n",
       " 'Ġdischarged': 26476,\n",
       " 'ĠHitchcock': 49817,\n",
       " 'éĢ': 34460,\n",
       " 'Ġmeltdown': 37824,\n",
       " 'Ġspores': 47306,\n",
       " 'idan': 27610,\n",
       " 'Param': 22973,\n",
       " 'Grad': 42731,\n",
       " 'ambo': 22651,\n",
       " 'ĠChile': 17456,\n",
       " 'ĠJaune': 29415,\n",
       " 'ĠMagic': 6139,\n",
       " 'Ġpunishment': 9837,\n",
       " 'Factor': 41384,\n",
       " 'General': 12218,\n",
       " 'ĠNTS': 43754,\n",
       " 'Fri': 30214,\n",
       " 'ĠJose': 5264,\n",
       " 'ĠPear': 11830,\n",
       " 'Ġdealers': 15737,\n",
       " 'ĠGarland': 35466,\n",
       " 'Ġfatig': 46291,\n",
       " 'Ġheels': 18728,\n",
       " 'Ġideally': 30274,\n",
       " 'STAT': 35744,\n",
       " 'ĠPierre': 21204,\n",
       " 'Ġfitting': 15830,\n",
       " 'ĠLat': 5476,\n",
       " 'Ġflav': 10525,\n",
       " 'Federal': 24099,\n",
       " 'elf': 7046,\n",
       " 'sb': 36299,\n",
       " 'win': 5404,\n",
       " 'Ġcurriculum': 20583,\n",
       " 'Ġmercy': 17703,\n",
       " 'Ġreference': 4941,\n",
       " 'Ġwiring': 29477,\n",
       " 'Ġpedoph': 27706,\n",
       " 'ĠJosh': 8518,\n",
       " 'Ġreminders': 40687,\n",
       " 'York': 49278,\n",
       " 'acht': 19725,\n",
       " 'ĠYoutube': 27431,\n",
       " 'Ġgroundwork': 40641,\n",
       " 'Ġhammered': 41223,\n",
       " 'Ġblatantly': 39340,\n",
       " 'Merc': 42981,\n",
       " '=]': 48874,\n",
       " 'ĠImp': 9855,\n",
       " 'Ġcircuit': 10349,\n",
       " 'Ġintersect': 36177,\n",
       " ':': 25,\n",
       " 'spe': 4125,\n",
       " 'Ġattm': 36926,\n",
       " 'alking': 18998,\n",
       " 'proxy': 36436,\n",
       " 'sense': 33819,\n",
       " 'ĠSNAP': 48592,\n",
       " 'Ġmas': 12422,\n",
       " 'Ġnow': 783,\n",
       " 'Ġoppression': 18345,\n",
       " 'fortune': 37359,\n",
       " 'internal': 32538,\n",
       " 'www': 2503,\n",
       " 'ĠHorus': 29698,\n",
       " 'Ġseen': 1775,\n",
       " 'Ġuber': 48110,\n",
       " '830': 48341,\n",
       " 'ļéĨĴ': 22757,\n",
       " 'iotic': 16357,\n",
       " 'Times': 28595,\n",
       " 'ĠLyons': 43728,\n",
       " '677': 40179,\n",
       " 'LV': 30976,\n",
       " 'ĠNeville': 35840,\n",
       " 'ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ': 14827,\n",
       " 'ĠReuters': 8428,\n",
       " 'through': 9579,\n",
       " 'die': 11979,\n",
       " 'ĠRuby': 10888,\n",
       " 'ĠTimes': 3782,\n",
       " 'Ġcellul': 47354,\n",
       " 'ĠHar': 2113,\n",
       " 'Ġfrontrunner': 49981,\n",
       " 'Ġtast': 14854,\n",
       " 'nw': 47516,\n",
       " 'ĠOlympics': 14935,\n",
       " 'Ġmatte': 36908,\n",
       " 'Ġdonation': 13784,\n",
       " 'Oper': 18843,\n",
       " 'osures': 16091,\n",
       " 'âĶģ': 47486,\n",
       " 'ĠIo': 27853,\n",
       " 'strong': 11576,\n",
       " 'Ġ&&': 11405,\n",
       " 'Ġoceans': 23744,\n",
       " 'aret': 8984,\n",
       " 'ĠMovement': 15477,\n",
       " 'ĠPharaoh': 42697,\n",
       " 'Â£': 14988,\n",
       " 'stress': 41494,\n",
       " 'ĠDU': 35480,\n",
       " 'Ġbarren': 39497,\n",
       " 'Ir': 23820,\n",
       " 'Ġbesie': 38128,\n",
       " 'Ġhabitat': 20018,\n",
       " 'Ġinsertion': 36075,\n",
       " 'Ġmaintain': 5529,\n",
       " 'mud': 41650,\n",
       " 'PT': 11571,\n",
       " 'allowed': 40845,\n",
       " 'ributes': 7657,\n",
       " 'ĠThailand': 16952,\n",
       " 'ĠReich': 24560,\n",
       " 'INTER': 41358,\n",
       " 'Ġindul': 19441,\n",
       " '207': 22745,\n",
       " 'nets': 45938,\n",
       " 'ibraries': 11127,\n",
       " 'ĠDrac': 35693,\n",
       " 'Ġhypothesized': 43714,\n",
       " 'Ġkitchen': 9592,\n",
       " 'Ġmayhem': 43744,\n",
       " 'Ġprec': 3718,\n",
       " 'Ġpreviews': 41418,\n",
       " 'Ġregener': 16935,\n",
       " '................................': 8864,\n",
       " 'Package': 27813,\n",
       " 'inia': 43168,\n",
       " 'Ġcob': 22843,\n",
       " 'ĠWarning': 15932,\n",
       " 'Ġoutcry': 34334,\n",
       " 'ĠLiqu': 35515,\n",
       " 'Ġion': 22088,\n",
       " 'bage': 13866,\n",
       " 'ĠTNT': 42490,\n",
       " 'Ġwills': 49928,\n",
       " 'Ġoccupied': 12030,\n",
       " 'Ġwreckage': 31781,\n",
       " 'ĠMoroc': 23711,\n",
       " 'qq': 38227,\n",
       " 'ĠTorch': 34868,\n",
       " 'eaturing': 31347,\n",
       " 'Ġqualifier': 39265,\n",
       " 'oga': 10949,\n",
       " 'Storm': 32173,\n",
       " 'pse': 7752,\n",
       " 'Ġfishes': 49765,\n",
       " '6666': 19060,\n",
       " 'robat': 40655,\n",
       " 'Ġgroundbreaking': 31088,\n",
       " 'ĠPoo': 41778,\n",
       " 'Ġonset': 21228,\n",
       " 'Ġtransgress': 44780,\n",
       " '778': 39761,\n",
       " '158': 21273,\n",
       " 'aby': 3930,\n",
       " 'aleigh': 30729,\n",
       " 'è': 164,\n",
       " 'ĠGamergate': 37337,\n",
       " 'tri': 28461,\n",
       " 'ĠGreeks': 25059,\n",
       " 'Play': 11002,\n",
       " 'Ġlarg': 2552,\n",
       " 'Ale': 37474,\n",
       " 'mic': 9383,\n",
       " 'Ġ163': 26826,\n",
       " 'Ġbrown': 7586,\n",
       " 'Slow': 36423,\n",
       " 'ĠIndones': 12638,\n",
       " 'Ġpity': 26246,\n",
       " 'ruary': 3728,\n",
       " 'ĠDhabi': 43941,\n",
       " 'ĠJian': 40922,\n",
       " '=\"': 2625,\n",
       " 'Ġheadquartered': 48583,\n",
       " 'Ġseekers': 22310,\n",
       " 'ateur': 15093,\n",
       " 'ĠInterestingly': 25044,\n",
       " 'Ġepis': 48177,\n",
       " 'Ġblasted': 26044,\n",
       " 'Ġè£ıè': 34504,\n",
       " 'sl': 6649,\n",
       " 'ĠIntellectual': 42443,\n",
       " 'Ġpriest': 11503,\n",
       " '780': 40873,\n",
       " 'alyst': 21470,\n",
       " '¿': 123,\n",
       " 'Ġoriginally': 6198,\n",
       " 'elligence': 3480,\n",
       " 'ĠMatt': 4705,\n",
       " 'Ġsocialist': 15889,\n",
       " 'Oracle': 48625,\n",
       " 'ĠNaruto': 22170,\n",
       " 'Ġbeneficiary': 33867,\n",
       " 'Summer': 33560,\n",
       " 'Ġantis': 33064,\n",
       " 'ĠPolice': 4287,\n",
       " 'oÄŁan': 48030,\n",
       " 'orest': 26522,\n",
       " 'Ġdesigns': 9824,\n",
       " 'Ġstatutes': 24895,\n",
       " 'Ġsustained': 12605,\n",
       " 'etta': 15253,\n",
       " 'Ag': 10262,\n",
       " 'å£': 18004,\n",
       " 'abol': 28426,\n",
       " 'Ġplotted': 37515,\n",
       " 'umann': 40062,\n",
       " 'Ġsprayed': 35729,\n",
       " 'ablished': 22555,\n",
       " 'afety': 27925,\n",
       " 'going': 5146,\n",
       " 'ĠPCs': 21869,\n",
       " 'Ġderivative': 27255,\n",
       " 'Ġenvironmentally': 34132,\n",
       " 'Ġ${': 25597,\n",
       " 'ĠKre': 25732,\n",
       " 'ĠMari': 29423,\n",
       " 'ĠLAPD': 44776,\n",
       " 'Ġ\"(': 30629,\n",
       " 'Ġridicule': 38364,\n",
       " 'Ġdrank': 24070,\n",
       " 'ĠShots': 42162,\n",
       " 'ãĥ¯': 25589,\n",
       " '--+': 44785,\n",
       " 'ĠKentucky': 11867,\n",
       " 'Ġdetermin': 3416,\n",
       " 'Ġrevolutionaries': 44347,\n",
       " 'Ġstockp': 29534,\n",
       " 'ĠRath': 26494,\n",
       " 'wage': 21482,\n",
       " 'ĠWednesday': 3583,\n",
       " 'ĠCSS': 17391,\n",
       " 'Ġcapac': 18457,\n",
       " 'Ġcherish': 48303,\n",
       " 'Ġheightened': 25556,\n",
       " 'Ġinvoice': 45458,\n",
       " 'Ġmisogyn': 22795,\n",
       " 'Ġmurd': 7847,\n",
       " '][/': 44926,\n",
       " 'ITY': 9050,\n",
       " 'Ġold': 1468,\n",
       " 'Ġoverflowing': 43347,\n",
       " 'ifted': 21715,\n",
       " 'ĠChaos': 13903,\n",
       " 'Ġvolunteers': 11661,\n",
       " 'ele': 11129,\n",
       " 'ĠCape': 15725,\n",
       " 'ĠNCAA': 15244,\n",
       " 'Ġhonors': 25279,\n",
       " 'lets': 5289,\n",
       " 'Ġdystop': 39227,\n",
       " 'ĠRaqqa': 30391,\n",
       " 'ĠThrust': 49794,\n",
       " 'Ġrelay': 24248,\n",
       " 'ĠHound': 44900,\n",
       " 'Ġstroke': 14000,\n",
       " 'ĠDoctrine': 35541,\n",
       " 'King': 15708,\n",
       " 'asel': 48038,\n",
       " 'ĠGomez': 33231,\n",
       " 'town': 12735,\n",
       " 'ĠWhit': 13183,\n",
       " 'Ġsweep': 16085,\n",
       " 'ĠMight': 24213,\n",
       " 'ĠGeneva': 20552,\n",
       " 'Ġexpecting': 12451,\n",
       " 'ĠOwn': 11744,\n",
       " 'Ġuniverses': 44751,\n",
       " 'Michael': 13256,\n",
       " 'Ġmatters': 6067,\n",
       " 'suff': 37333,\n",
       " 'independence': 39894,\n",
       " 'ccording': 2941,\n",
       " 'irlfriend': 9872,\n",
       " 'Ġ226': 31510,\n",
       " 'ĠDreams': 21527,\n",
       " 'adal': 31682,\n",
       " 'llular': 32771,\n",
       " 'ĠStatus': 12678,\n",
       " 'ĠDetention': 46193,\n",
       " 'Ġdel': 1619,\n",
       " 'Ġbaths': 49076,\n",
       " 'Ġpale': 14005,\n",
       " 'Nev': 43555,\n",
       " 'six': 19412,\n",
       " 'ĠBolt': 21764,\n",
       " 'Ġarmoured': 44138,\n",
       " 'Ġdevil': 17118,\n",
       " 'Ġhydro': 17173,\n",
       " 'Ġpole': 16825,\n",
       " 'Ġqualitative': 40693,\n",
       " 'lies': 13508,\n",
       " 'ĠAdapt': 30019,\n",
       " 'ĠGirls': 12002,\n",
       " 'Ġfragmented': 41630,\n",
       " 'Ġinvests': 49862,\n",
       " 'Ġrailing': 47590,\n",
       " 'Ġreactors': 28502,\n",
       " 'Service': 16177,\n",
       " 'ĠCharge': 20260,\n",
       " 'Ġsandwic': 26155,\n",
       " 'Ġ229': 31064,\n",
       " 'Ġexplain': 4727,\n",
       " 'Ġfandom': 35387,\n",
       " 'Ġmuscle': 8280,\n",
       " 'BC': 2749,\n",
       " 'ĠSix': 9699,\n",
       " 'Ġfil': 1226,\n",
       " 'field': 3245,\n",
       " ']),': 46570,\n",
       " 'Ġundue': 38826,\n",
       " '221': 26115,\n",
       " 'compatible': 38532,\n",
       " 'Ġimply': 20135,\n",
       " 'ĠSophia': 39953,\n",
       " 'Alt': 29161,\n",
       " 'Ġlikewise': 20467,\n",
       " 'Ġperpetual': 29079,\n",
       " 'Ġsphere': 16558,\n",
       " 'Ġdetractors': 45642,\n",
       " 'Ġ171': 28369,\n",
       " 'ĠBas': 6455,\n",
       " 'piring': 12987,\n",
       " 'Ġimpartial': 32521,\n",
       " '1972': 41023,\n",
       " 'annie': 42883,\n",
       " 'ĠMiguel': 29825,\n",
       " 'ĠDome': 31390,\n",
       " 'Ġitiner': 45142,\n",
       " 'Ġpropulsion': 38873,\n",
       " 'armed': 12026,\n",
       " 'Ġactresses': 49798,\n",
       " 'ĠNare': 27236,\n",
       " 'Ġeffected': 43495,\n",
       " 'Ġ272': 38107,\n",
       " 'Ġnineteen': 43063,\n",
       " 'obyl': 46666,\n",
       " 'graded': 21791,\n",
       " 'ë': 167,\n",
       " 'Integ': 34500,\n",
       " 'Ġdeforestation': 48152,\n",
       " 'Ġninety': 37989,\n",
       " 'Ġportal': 17898,\n",
       " 'Ġvanish': 39572,\n",
       " 'Ġblinked': 40360,\n",
       " 'jing': 49940,\n",
       " 'ĠPrism': 35417,\n",
       " 'ausible': 17178,\n",
       " 'ictions': 9278,\n",
       " 'imposed': 36457,\n",
       " 'ote': 1258,\n",
       " 'Ġ768': 46720,\n",
       " 'Ġlords': 33200,\n",
       " 'harm': 29155,\n",
       " '229': 23539,\n",
       " 'ILL': 8267,\n",
       " 'Ġgets': 3011,\n",
       " 'Ġlove': 1842,\n",
       " 'Ġpaid': 3432,\n",
       " 'Ġredistribution': 41425,\n",
       " 'Ġstripped': 18818,\n",
       " 'Benz': 42484,\n",
       " 'ĠLGBTQ': 25172,\n",
       " 'Care': 17784,\n",
       " 'Node': 19667,\n",
       " 'Ġdiagrams': 37067,\n",
       " 'ln': 18755,\n",
       " 'ĠElven': 40748,\n",
       " 'ifferent': 17125,\n",
       " 'ĠAdvice': 42708,\n",
       " 'ĠNovel': 24467,\n",
       " 'Ġaloud': 32227,\n",
       " 'Ġbung': 43974,\n",
       " 'ĠGOT': 41725,\n",
       " 'sf': 28202,\n",
       " 'Ġoldest': 13325,\n",
       " 'Ġthinkers': 30504,\n",
       " 'ensen': 18756,\n",
       " 'violent': 24498,\n",
       " 'ĠGR': 10863,\n",
       " 'Ġweekly': 10273,\n",
       " 'Ġatro': 38324,\n",
       " 'Ġtyped': 25683,\n",
       " 'Ġitself': 2346,\n",
       " 'Ġchecklist': 41859,\n",
       " 'INST': 38604,\n",
       " 'Ġsolicitor': 48709,\n",
       " 'Ġhtt': 1841,\n",
       " 'ĠNeck': 34726,\n",
       " 'Ġmankind': 18019,\n",
       " 'ĠIranian': 10704,\n",
       " 'ĠBron': 8923,\n",
       " 'Ġdivine': 11871,\n",
       " 'Ġpragmatic': 33119,\n",
       " 'Ġsavage': 27303,\n",
       " 'aders': 9972,\n",
       " 'Ġ1944': 16994,\n",
       " 'pired': 6474,\n",
       " 'à': 156,\n",
       " 'ĠCourse': 20537,\n",
       " 'ath': 776,\n",
       " 'tax': 19290,\n",
       " 'ĠBack': 5157,\n",
       " 'HF': 29567,\n",
       " 'ĠAR': 5923,\n",
       " 'ĠProc': 31345,\n",
       " '2012': 6999,\n",
       " 'ĠProdu': 21522,\n",
       " 'Ġlauded': 42086,\n",
       " 'Ġradio': 5243,\n",
       " 'ĠXan': 47482,\n",
       " 'Ġsymm': 23606,\n",
       " 'Ġpurs': 8012,\n",
       " 'ivers': 1191,\n",
       " 'roph': 10051,\n",
       " 'ĠCrowley': 35332,\n",
       " 'Ġheats': 37876,\n",
       " 'ÑĢ': 21169,\n",
       " '____': 1427,\n",
       " 'ĠDeal': 15138,\n",
       " 'Ġ:=': 19039,\n",
       " 'ĠNIC': 45593,\n",
       " 'Ġpress': 1803,\n",
       " 'Ġrepetitive': 28585,\n",
       " 'ĠShinra': 47981,\n",
       " 'Ġtransmit': 21937,\n",
       " 'inspired': 24194,\n",
       " 'pir': 4063,\n",
       " 'idents': 3231,\n",
       " 'Ġhi': 23105,\n",
       " 'Ġhorsepower': 36696,\n",
       " 'Ġreflecting': 20252,\n",
       " 'ieft': 49868,\n",
       " 'Ġ122': 19409,\n",
       " 'Ġvacancies': 48639,\n",
       " 'ĠAwoken': 8755,\n",
       " 'phen': 31024,\n",
       " 'ĠController': 22741,\n",
       " 'Ġdisg': 13757,\n",
       " 'Ġfors': 46931,\n",
       " 'Ġstunts': 49772,\n",
       " 'ĠTerra': 24118,\n",
       " 'Ġblaming': 24630,\n",
       " 'Ġexperimented': 42107,\n",
       " 'Ġrecognised': 20915,\n",
       " 'Ġfreeing': 35800,\n",
       " 'Ġrecycle': 48914,\n",
       " 'DERR': 49643,\n",
       " 'Ġaccur': 4431,\n",
       " 'Ġcorrelated': 23551,\n",
       " 'Ġsuspicion': 15123,\n",
       " 'Sent': 31837,\n",
       " 'ometric': 16996,\n",
       " 'ĠVader': 27403,\n",
       " 'Ġide': 1405,\n",
       " 'Ġtumult': 38064,\n",
       " '806': 37988,\n",
       " 'ĠLucy': 22162,\n",
       " 'Ġreduces': 12850,\n",
       " 'Ġdismissive': 49010,\n",
       " 'Ġtextures': 20028,\n",
       " 'ocrine': 38658,\n",
       " 'Ġaspiration': 48217,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看词汇表\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8be0bd1-c9c0-4cb2-92fa-0b8c5d230d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for 'world': 6894\n",
      "Token for ID 995: Ġworld\n"
     ]
    }
   ],
   "source": [
    "# 查看特定 Token 的 ID\n",
    "token_id = tokenizer.convert_tokens_to_ids('world')\n",
    "print(\"Token ID for 'world':\", token_id)\n",
    "\n",
    "# 查看特定 ID 对应的 Token\n",
    "token = tokenizer.convert_ids_to_tokens(995)\n",
    "print(\"Token for ID 995:\", token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c3d36-3ac7-40ef-a8f7-48277b066825",
   "metadata": {},
   "source": [
    "> 这里的 `Ġ` 代表一个空格字符：\n",
    ">\n",
    "> ```python\n",
    "> print(tokenizer.tokenize(' '))\n",
    "> ```\n",
    ">\n",
    "> 输出为 `['Ġ']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26fc1035-abf5-46de-923a-e082863b55c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Special Tokens: ['<|endoftext|>']\n",
      "Special Token IDs: [50256]\n"
     ]
    }
   ],
   "source": [
    "# 查看所有特殊标记\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "print(\"All Special Tokens:\", special_tokens)\n",
    "\n",
    "# 查看特殊标记对应的 ID\n",
    "special_token_ids = tokenizer.all_special_ids\n",
    "print(\"Special Token IDs:\", special_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c39bd-fd98-46c4-9da7-06f08f327125",
   "metadata": {},
   "source": [
    "**「接下来，我们将探讨 Tokenizer 的具体细节」**\n",
    "\n",
    "> 不需要深入接下来的所有代码细节，只需要查看输出与相应「步骤」的表述。\n",
    "\n",
    "## 分词（Tokenize）\n",
    "\n",
    "我们需要将语料库（corpus）的文本拆分为单词，假设当前语料库包含的单词和对应频次如下：\n",
    "\n",
    "```sql\n",
    "(\"low\", 5), (\"lower\", 2), (\"newest\", 6), (\"widest\", 3)\n",
    "```\n",
    "\n",
    "有些论文也用 `vocab` 来表述，知道后面是频次即可，命名不用纠结。\n",
    "\n",
    "### 构造词汇表\n",
    "\n",
    "#### Byte-Pair Encoding (BPE)\n",
    "\n",
    "> **参考文献：**\n",
    ">\n",
    "> - [A new algorithm for data compression. 1994](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\n",
    "> - [Neural Machine Translation of Rare Words with Subword Units. 2015](https://arxiv.org/pdf/1508.07909v5)\n",
    ">\n",
    "> BPE 是一种基于数据压缩的技术，最早由 Gage 在 1994 年提出，后来被用于 GPT 等模型。它是一种子词分割算法，从字符级别开始，通过迭代合并频率最高的字符对（或字符序列）来构建新的 Token，从而可以处理部分 OOV（Out-Of-Vocabulary）情况。\n",
    ">\n",
    "> **Q: 什么是 OOV ？**\n",
    ">\n",
    "> 其实就是不在词汇表中的词，也称之为「未登录词」。\n",
    "\n",
    "BPE 每次的迭代目标是找到频率最高的相邻字符对，定义 Score 以与 WordPiece 作对比：\n",
    "\n",
    "$$\n",
    "\\text{Score}_{\\text{BPE}}(x, y) = \\text{freq}(x, y)\n",
    "$$\n",
    "其中，$\\text{freq}(x, y)$ 表示字符对 $(x, y)$ 在语料库中的出现频次。\n",
    "\n",
    "##### 步骤\n",
    "\n",
    "1. **初始化词汇表 $V$**：\n",
    "   - $V$ 包含语料库中的所有唯一字符，即单词字符的集合。\n",
    "2. **统计字符对的频次**：\n",
    "   - 对于每个单词的字符序列，统计相邻字符对的出现频次。\n",
    "3. **找到频次（Score）最高的字符对并合并**：\n",
    "   - 选择出现频率最高的字符对 $(x, y)$，将其合并为新符号 $xy$。\n",
    "4. **更新词汇表并重复步骤 2 到 4**：\n",
    "   - 将新符号添加到词汇表 $V = V \\cup \\{xy\\}$。\n",
    "   - 更新语料库中的单词表示，重复统计和合并过程，直到满足停止条件（例如，词汇表达到预定大小）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc09234-cef2-4898-8260-3ce47e0b2d50",
   "metadata": {},
   "source": [
    "##### 示例\n",
    "\n",
    "**步骤 1：初始化词汇表**\n",
    "\n",
    "- **将单词拆分为字符序列**：\n",
    "\n",
    "  ```plaintext\n",
    "  (\"l\", \"o\", \"w\"), 5  \n",
    "  (\"l\", \"o\", \"w\", \"e\", \"r\"), 2  \n",
    "  (\"n\", \"e\", \"w\", \"e\", \"s\", \"t\"), 6  \n",
    "  (\"w\", \"i\", \"d\", \"e\", \"s\", \"t\"), 3\n",
    "  ```\n",
    "\n",
    "- **词汇表 $V$**：\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd'}\n",
    "  ```\n",
    "\n",
    "**步骤 2：统计字符对的频次**\n",
    "\n",
    "编写一个函数，根据给定的单词和其频次，自动统计字符对的频次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39dfe4fe-c1fd-4c54-aa9b-3b21810b109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字符对频次统计结果:\n",
      "('l', 'o'): 7\n",
      "('o', 'w'): 7\n",
      "('w', 'e'): 8\n",
      "('e', 'r'): 2\n",
      "('n', 'e'): 6\n",
      "('e', 'w'): 6\n",
      "('e', 's'): 9\n",
      "('s', 't'): 9\n",
      "('w', 'i'): 3\n",
      "('i', 'd'): 3\n",
      "('d', 'e'): 3\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_char_pairs(word_freq):\n",
    "    \"\"\"\n",
    "    计算字符对的频次。\n",
    "    \n",
    "    参数：\n",
    "    \tword_freq: List of tuples, 每个元组包含单词和其频次\n",
    "    \t\n",
    "    返回：\n",
    "\t    字符对频次的字典\n",
    "    \"\"\"\n",
    "    pair_freq = defaultdict(int)\n",
    "    for word, freq in word_freq:\n",
    "        chars = list(word)\n",
    "        for i in range(len(chars) - 1):\n",
    "            pair = (chars[i], chars[i + 1])\n",
    "            pair_freq[pair] += freq\n",
    "    return pair_freq\n",
    "\n",
    "# 示例词汇表和单词频次\n",
    "word_freq = [\n",
    "    (\"low\", 5),\n",
    "    (\"lower\", 2),\n",
    "    (\"newest\", 6),\n",
    "    (\"widest\", 3)\n",
    "]\n",
    "\n",
    "pair_freq = count_char_pairs(word_freq)\n",
    "print(\"字符对频次统计结果:\")\n",
    "for pair, freq in pair_freq.items():\n",
    "    print(f\"{pair}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c237d666-d360-44b3-95d8-54b56884cb5d",
   "metadata": {},
   "source": [
    "**步骤 3：找到频次最高的字符对并合并**\n",
    "\n",
    "- **选择频次最高的字符对**：\n",
    "\n",
    "  - `(\"e\", \"s\")` 和 `(\"s\", \"t\")`，频次均为 9。可以任选其一进行合并，假设选择排序第一的： `(\"e\", \"s\")`。\n",
    "\n",
    "- **合并 `(\"e\", \"s\")` 为新符号 `es`**。\n",
    "\n",
    "- **记录合并操作**：\n",
    "\n",
    "  ```plaintext\n",
    "  Merge 1: (\"e\", \"s\") -> \"es\"\n",
    "  ```\n",
    "\n",
    "**步骤 4：更新词汇表并重复**\n",
    "\n",
    "- **更新单词序列**：\n",
    "\n",
    "  ```plaintext\n",
    "  (\"l\", \"o\", \"w\"), 5  \n",
    "  (\"l\", \"o\", \"w\", \"e\", \"r\"), 2  \n",
    "  (\"n\", \"e\", \"w\", \"es\", \"t\"), 6  \n",
    "  (\"w\", \"i\", \"d\", \"es\", \"t\"), 3\n",
    "  ```\n",
    "\n",
    "- **更新词汇表 $V$**：\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es'}\n",
    "  ```\n",
    "\n",
    "- **重复步骤 2 到 4，直到达到预定的词汇表大小**。\n",
    "\n",
    "> ##### 📝 练习题\n",
    ">\n",
    "> 停下来思考一下，答案和代码位于当前模块末尾。\n",
    ">\n",
    "> **Q1.** 最初的词汇表大小为 10，假设预定大小为 13，那么当前的词汇表 $V$ 为多少？合并记录是什么？\n",
    ">\n",
    "> **Q2.** 如果以 `</w>`（表示单词结尾）作为每个语料库中单词的结尾，最初的词汇表会受到什么影响，后续的过程会如何变化？假设预定大小为 14，当前的合并记录是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe352af-0f8f-4848-a8dd-b10136fbe205",
   "metadata": {},
   "source": [
    "#### WordPiece\n",
    "\n",
    "> **参考文献：**\n",
    ">\n",
    "> - [Japanese and Korean voice search. 2012](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/37842.pdf)\n",
    "> - [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. 2016](https://arxiv.org/pdf/1609.08144v2)\n",
    ">\n",
    "> WordPiece 是一种子词分割算法，最初用于处理日语和韩语的语音搜索，后来在 Google 的神经机器翻译系统中得到应用。\n",
    "\n",
    "与 BPE 不同，WordPiece 的 Score 由字符对频次与其组成部分频次的比值决定，定义 Score：\n",
    "\n",
    "$$\n",
    "\\text{Score}_{\\text{WordPiece}}(x, y) = \\frac{\\text{freq}(xy)}{\\text{freq}(x) \\times \\text{freq}(y)}\n",
    "$$\n",
    "\n",
    "其中，$\\text{freq}(x)$、$\\text{freq}(y)$ 和 $\\text{freq}(xy)$ 分别表示符号 $x$、$y$ 和它们合并后的符号 $xy$ 的频次。\n",
    "\n",
    "##### 步骤\n",
    "\n",
    "1. **初始化词汇表 $V$**：\n",
    "   - 与 BPE 相同，$V$ 包含语料库中的所有唯一字符，但处理方式略有不同：对于每个单词，除了首个字符外，其他字符前都加上 `##` 前缀。\n",
    "2. **统计字符对的频次及 Score**：\n",
    "   - 对于每个可能的字符对 $(x, y)$，计算 $\\text{freq}(x)$、$\\text{freq}(y)$、$\\text{freq}(xy)$，并计算 Score。\n",
    "3. **找到 Score 最高的字符对并合并**：\n",
    "   - 选择 Score 最高的字符对 $(x, y)$，将其合并为新符号 $xy$，注意：\n",
    "     - 如果第二个符号以 `##` 开头，合并时去掉 `##` 前缀再进行连接。\n",
    "     - 新符号是否以 `##` 开头，取决于第一个符号是否以 `##` 开头。\n",
    "4. **更新词汇表并重复步骤 2 到 4**：\n",
    "   - 将新符号添加到词汇表 $V = V \\cup \\{xy\\}$。\n",
    "   - 更新语料库中的单词表示，重复统计和合并过程，直到满足停止条件。\n",
    "\n",
    "##### 示例\n",
    "\n",
    "使用与 BPE 示例相同的语料库。\n",
    "\n",
    "**步骤 1：初始化词汇表**\n",
    "\n",
    "- **将单词拆分为字符序列**：\n",
    "\n",
    "  ```plaintext\n",
    "  ('l', '##o', '##w'), 5                       # \"low\"\n",
    "  ('l', '##o', '##w', '##e', '##r'), 2         # \"lower\"\n",
    "  ('n', '##e', '##w', '##e', '##s', '##t'), 6  # \"newest\"\n",
    "  ('w', '##i', '##d', '##e', '##s', '##t'), 3  # \"widest\"\n",
    "  ```\n",
    "\n",
    "- **词汇表 $V$**：\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', '##o', '##w', '##e', '##r', 'n', '##s', '##t', 'w', '##i', '##d'}\n",
    "  ```\n",
    "\n",
    "**步骤 2：统计字符和字符对的频次，计算 Score**\n",
    "\n",
    "可以设计一个函数完成这个步骤（直接运行查看输出）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5910b8ee-7091-4cd6-803a-73802b7e5d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字符对频次统计结果:\n",
      "('l', '##o'): 7\n",
      "('##o', '##w'): 7\n",
      "('##w', '##e'): 8\n",
      "('##e', '##r'): 2\n",
      "('n', '##e'): 6\n",
      "('##e', '##w'): 6\n",
      "('##e', '##s'): 9\n",
      "('##s', '##t'): 9\n",
      "('w', '##i'): 3\n",
      "('##i', '##d'): 3\n",
      "('##d', '##e'): 3\n",
      "\n",
      "单个字符频次统计结果:\n",
      "l: 7\n",
      "##o: 7\n",
      "##w: 13\n",
      "##e: 17\n",
      "##r: 2\n",
      "n: 6\n",
      "##s: 9\n",
      "##t: 9\n",
      "w: 3\n",
      "##i: 3\n",
      "##d: 3\n",
      "\n",
      "字符对 Score 计算结果:\n",
      "('l', '##o'): 0.1429\n",
      "('##o', '##w'): 0.0769\n",
      "('##w', '##e'): 0.0362\n",
      "('##e', '##r'): 0.0588\n",
      "('n', '##e'): 0.0588\n",
      "('##e', '##w'): 0.0271\n",
      "('##e', '##s'): 0.0588\n",
      "('##s', '##t'): 0.1111\n",
      "('w', '##i'): 0.3333\n",
      "('##i', '##d'): 0.3333\n",
      "('##d', '##e'): 0.0588\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_char_pairs_wordpiece(word_freq):\n",
    "    \"\"\"\n",
    "    计算字符对的频次和单个字符的频次。\n",
    "    \n",
    "    参数：\n",
    "\t\tword_freq: List of tuples, 每个元组包含单词（列表形式）和其频次\n",
    "\t\t\n",
    "\t返回：\n",
    "\t\t两个字典，分别为字符对频次和单个字符频次\n",
    "    \"\"\"\n",
    "    pair_freq = defaultdict(int)\n",
    "    char_freq = defaultdict(int)\n",
    "    for word, freq in word_freq:\n",
    "        for i in range(len(word)):\n",
    "            char_freq[word[i]] += freq\n",
    "            if i < len(word) - 1:\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pair_freq[pair] += freq\n",
    "    return pair_freq, char_freq\n",
    "\n",
    "def compute_wordpiece_score(freq_xy, freq_x, freq_y):\n",
    "    \"\"\"\n",
    "    根据 WordPiece 的定义计算 Score。\n",
    "    \n",
    "    参数：\n",
    "\t\tfreq_xy: 符号对的频次\n",
    "\t\tfreq_x: 符号 x 的频次\n",
    "\t\tfreq_y: 符号 y 的频次\n",
    "\t\t\n",
    "\t返回：\n",
    "\t\t计算得到的 Score\n",
    "    \"\"\"\n",
    "    if freq_x == 0 or freq_y == 0:\n",
    "        return 0\n",
    "    return freq_xy / (freq_x * freq_y)\n",
    "\n",
    "# 示例词汇表和单词频次\n",
    "word_freq = [\n",
    "    (['l', '##o', '##w'], 5),\n",
    "    (['l', '##o', '##w', '##e', '##r'], 2),\n",
    "    (['n', '##e', '##w', '##e', '##s', '##t'], 6),\n",
    "    (['w', '##i', '##d', '##e', '##s', '##t'], 3)\n",
    "]\n",
    "\n",
    "# 统计字符对频次和单个字符频次\n",
    "pair_freq, char_freq = count_char_pairs_wordpiece(word_freq)\n",
    "\n",
    "# 计算每对字符的 Score\n",
    "scores = {}\n",
    "for pair in pair_freq:\n",
    "    freq_xy = pair_freq[pair]\n",
    "    freq_x = char_freq[pair[0]]\n",
    "    freq_y = char_freq[pair[1]]\n",
    "    score = compute_wordpiece_score(freq_xy, freq_x, freq_y)\n",
    "    scores[pair] = score\n",
    "\n",
    "# 输出结果\n",
    "print(\"字符对频次统计结果:\")\n",
    "for pair, freq in pair_freq.items():\n",
    "    print(f\"{pair}: {freq}\")\n",
    "\n",
    "print(\"\\n单个字符频次统计结果:\")\n",
    "for char, freq in char_freq.items():\n",
    "    print(f\"{char}: {freq}\")\n",
    "\n",
    "print(\"\\n字符对 Score 计算结果:\")\n",
    "for pair, score in scores.items():\n",
    "    print(f\"{pair}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204f92b-08d1-42b5-a3b1-1b38a2fa2a3f",
   "metadata": {},
   "source": [
    "- **选择频次最高的字符对**：\n",
    "\n",
    "  -  `('w', '##i')` 和 `('##i', '##d')`，Score 都为 0.3333。可以任选其一进行合并，假设选择排序第一的： `(\"w\", \"##i\")`。\n",
    "\n",
    "- **合并 `('w', '##i')` 为新符号 `wi`**\n",
    "\n",
    "  - 注意：合并时，若第二个符号以 `##` 开头，合并后的新符号为第一个符号加上第二个符号去掉 `##` 前缀的部分。\n",
    "\n",
    "- **记录合并操作：**\n",
    "\n",
    "  ```plaintext\n",
    "  Merge 1: ('w', '##i') -> 'wi'\n",
    "  ```\n",
    "\n",
    "**步骤 4：更新词汇表并重复**\n",
    "\n",
    "- **更新词汇表 $V$**：\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', '##o', '##w', '##e', '##r', 'n', '##s', '##t', 'w', '##i', '##d', 'wi'}\n",
    "  ```\n",
    "\n",
    "- **更新单词序列**：\n",
    "\n",
    "  ```plaintext\n",
    "  ('l', '##o', '##w'), 5                       # \"low\"\n",
    "  ('l', '##o', '##w', '##e', '##r'), 2         # \"lower\"\n",
    "  ('n', '##e', '##w', '##e', '##s', '##t'), 6  # \"newest\"\n",
    "  ('wi', '##d', '##e', '##s', '##t'), 3        # \"widest\"\n",
    "  ```\n",
    "\n",
    "- **重复步骤 2 到 4，直到达到预定的词汇表大小**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125f661-a493-405f-8c2e-cee84129afbe",
   "metadata": {},
   "source": [
    "##### 使用函数实现简单的 WordPiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b5a44a-72b1-46f5-997e-b497940186fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge: ('w', '##i') -> wi, Score: 0.3333, 词汇表大小: 12\n",
      "Merge: ('wi', '##d') -> wid, Score: 0.3333, 词汇表大小: 13\n",
      "Merge: ('l', '##o') -> lo, Score: 0.1429, 词汇表大小: 14\n",
      "Merge: ('##s', '##t') -> ##st, Score: 0.1111, 词汇表大小: 15\n",
      "\n",
      "最终词汇表 V:\n",
      "{'##o', '##st', 'lo', '##r', 'n', '##s', 'wid', 'w', 'wi', '##i', '##e', '##w', 'l', '##d', '##t'}\n",
      "\n",
      "合并记录:\n",
      "Merge 1: ('w', '##i') -> wi\n",
      "Merge 2: ('wi', '##d') -> wid\n",
      "Merge 3: ('l', '##o') -> lo\n",
      "Merge 4: ('##s', '##t') -> ##st\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_new_symbol(x, y):\n",
    "    \"\"\"\n",
    "    根据 WordPiece 的规则创建新符号。\n",
    "\n",
    "    - 如果 y 以 '##' 开头，合并时需要去掉 y 的 '##' 前缀。\n",
    "    - 新符号是否以 '##' 开头，取决于 x 是否以 '##' 开头。\n",
    "    \"\"\"\n",
    "    x_starts_hash = x.startswith('##')\n",
    "    x_without_hash = x[2:] if x_starts_hash else x\n",
    "    y_without_hash = y[2:] if y.startswith('##') else y\n",
    "    new_symbol = x_without_hash + y_without_hash\n",
    "    if x_starts_hash:\n",
    "        new_symbol = '##' + new_symbol\n",
    "    return new_symbol\n",
    "\n",
    "def count_char_pairs_wordpiece(word_freq):\n",
    "    \"\"\"\n",
    "    计算字符对的频次和单个字符的频次。\n",
    "    \n",
    "    参数：\n",
    "\t\tword_freq: List of tuples, 每个元组包含单词（列表形式）和其频次\n",
    "\t\t\n",
    "\t返回：\n",
    "\t\t两个字典，分别为字符对频次和单个字符频次\n",
    "    \"\"\"\n",
    "    pair_freq = defaultdict(int)\n",
    "    char_freq = defaultdict(int)\n",
    "    for word, freq in word_freq:\n",
    "        for i in range(len(word)):\n",
    "            char_freq[word[i]] += freq\n",
    "            if i < len(word) - 1:\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pair_freq[pair] += freq\n",
    "    return pair_freq, char_freq\n",
    "\n",
    "def compute_wordpiece_score(freq_xy, freq_x, freq_y):\n",
    "    \"\"\"\n",
    "    根据 WordPiece 的定义计算 Score。\n",
    "    \n",
    "    参数：\n",
    "\t\tfreq_xy: 符号对的频次\n",
    "\t\tfreq_x: 符号 x 的频次\n",
    "\t\tfreq_y: 符号 y 的频次\n",
    "\t\t\n",
    "\t返回：\n",
    "\t\t计算得到的 Score\n",
    "    \"\"\"\n",
    "    if freq_x == 0 or freq_y == 0:\n",
    "        return 0\n",
    "    return freq_xy / (freq_x * freq_y)\n",
    "\n",
    "def find_best_pair_wordpiece(pair_freq, char_freq):\n",
    "    \"\"\"\n",
    "    找到具有最高 Score 的字符对。\n",
    "\n",
    "\t参数：\n",
    "\t\tpair_freq: 字符对频次的字典\n",
    "\t\tchar_freq: 单个字符频次的字典\n",
    "\t\t\n",
    "\t返回：\n",
    "\t\t具有最高 Score 的字符对及其 Score\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for pair, freq_xy in pair_freq.items():\n",
    "        x, y = pair\n",
    "        freq_x = char_freq.get(x, 0)\n",
    "        freq_y = char_freq.get(y, 0)\n",
    "        score = compute_wordpiece_score(freq_xy, freq_x, freq_y)\n",
    "        scores[pair] = score\n",
    "    if not scores:\n",
    "        return None, 0\n",
    "    best_pair = max(scores, key=scores.get)\n",
    "    return best_pair, scores[best_pair]\n",
    "\n",
    "def merge_pair_wordpiece(word_freq, pair_to_merge):\n",
    "    \"\"\"\n",
    "    合并指定的字符对到新符号。\n",
    "\n",
    "\t参数：\n",
    "\t\tword_freq: List of tuples, 每个元组包含单词（列表形式）和其频次\n",
    "\t\tpair_to_merge: 要合并的字符对\n",
    "\t返回：\n",
    "\t\t更新后的单词频次列表\n",
    "    \"\"\"\n",
    "    merged_word_freq = []\n",
    "    new_symbol = create_new_symbol(pair_to_merge[0], pair_to_merge[1])\n",
    "    for word, freq in word_freq:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            # 检查当前字符和下一个字符是否是要合并的字符对\n",
    "            if (\n",
    "                i < len(word) - 1\n",
    "                and word[i] == pair_to_merge[0]\n",
    "                and word[i + 1] == pair_to_merge[1]\n",
    "            ):\n",
    "                new_word.append(new_symbol)\n",
    "                i += 2  # 跳过下一个字符，因为已合并\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        merged_word_freq.append((new_word, freq))\n",
    "    return merged_word_freq\n",
    "\n",
    "def wordpiece_merge(word_freq, vocab_size):\n",
    "    \"\"\"\n",
    "    执行 WordPiece 合并操作，直到词汇表达到预定大小。\n",
    "\n",
    "\t参数：\n",
    "\t\tword_freq: List of tuples, 每个元组包含单词（列表形式）和其频次\n",
    "\t\tvocab_size: 预定的词汇表大小\n",
    "\t\t\n",
    "\t返回：\n",
    "\t\t最终词汇表和合并记录\n",
    "    \"\"\"\n",
    "    # 初始化词汇表\n",
    "    vocab = set()\n",
    "    for word, _ in word_freq:\n",
    "        vocab.update(word)\n",
    "    merges = []\n",
    "\n",
    "    while len(vocab) < vocab_size:\n",
    "        pair_freq, char_freq = count_char_pairs_wordpiece(word_freq)\n",
    "        best_pair, best_score = find_best_pair_wordpiece(pair_freq, char_freq)\n",
    "        if not best_pair:\n",
    "            break\n",
    "        # 合并最佳字符对\n",
    "        new_symbol = create_new_symbol(best_pair[0], best_pair[1])\n",
    "        word_freq = merge_pair_wordpiece(word_freq, best_pair)\n",
    "        vocab.add(new_symbol)\n",
    "        merges.append((best_pair, new_symbol))\n",
    "        print(\n",
    "            f\"Merge: {best_pair} -> {new_symbol}, Score: {best_score:.4f}, 词汇表大小: {len(vocab)}\"\n",
    "        )\n",
    "\n",
    "    return vocab, merges\n",
    "\n",
    "# 示例\n",
    "word_freq = [\n",
    "    (['l', '##o', '##w'], 5),\n",
    "    (['l', '##o', '##w', '##e', '##r'], 2),\n",
    "    (['n', '##e', '##w', '##e', '##s', '##t'], 6),\n",
    "    (['w', '##i', '##d', '##e', '##s', '##t'], 3)\n",
    "]\n",
    "\n",
    "# 预定词汇表大小为15\n",
    "final_vocab_wp, merge_records_wp = wordpiece_merge(word_freq, 15)\n",
    "\n",
    "print(\"\\n最终词汇表 V:\")\n",
    "print(final_vocab_wp)\n",
    "\n",
    "print(\"\\n合并记录:\")\n",
    "for idx, (pair, new_sym) in enumerate(merge_records_wp, 1):\n",
    "    print(f\"Merge {idx}: {pair} -> {new_sym}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a4da0-76fa-49b3-a327-525a47240d17",
   "metadata": {},
   "source": [
    "#### 📝 练习题答案\n",
    "\n",
    "**Q1. 最初的词汇表大小为 10，假设预定大小为 13，那么当前的词汇表 $V$ 为多少？合并记录呢？**\n",
    "\n",
    "- **初始词汇表 $V$**：\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd'}\n",
    "  ```\n",
    "\n",
    "  大小为 10。\n",
    "\n",
    "- **合并记录**：\n",
    "\n",
    "  1. 合并 `(\"e\", \"s\")` -> `es`，词汇表大小增加到 11。\n",
    "  2. 合并 `(\"es\", \"t\")` -> `est`，词汇表大小增加到 12。\n",
    "  3. 合并 `(\"l\", \"o\")` -> `lo`，词汇表大小增加到 13。\n",
    "\n",
    "- **最终词汇表 $V$**：\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es', 'est', 'lo'}\n",
    "  ```\n",
    "\n",
    "运行代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b1dd5a-2c25-467b-ba37-92ceed36df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge: ('e', 's') -> es, 词汇表大小: 11\n",
      "Merge: ('es', 't') -> est, 词汇表大小: 12\n",
      "Merge: ('l', 'o') -> lo, 词汇表大小: 13\n",
      "\n",
      "最终词汇表 V:\n",
      "{'lo', 'es', 'n', 'r', 'e', 'est', 'w', 'i', 'o', 'l', 's', 't', 'd'}\n",
      "\n",
      "合并记录:\n",
      "Merge 1: ('e', 's') -> es\n",
      "Merge 2: ('es', 't') -> est\n",
      "Merge 3: ('l', 'o') -> lo\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_char_pairs(word_freq):\n",
    "    \"\"\"\n",
    "    计算字符对的频次。\n",
    "    \n",
    "    参数：\n",
    "\t    word_freq: List of tuples, 每个元组包含单词（列表形式）和其频次\n",
    "    \n",
    "    返回：\n",
    "    \t字符对频次的字典\n",
    "    \"\"\"\n",
    "    pair_freq = defaultdict(int)\n",
    "    for word, freq in word_freq:\n",
    "        for i in range(len(word) - 1):\n",
    "            pair = (word[i], word[i + 1])\n",
    "            pair_freq[pair] += freq\n",
    "    return pair_freq\n",
    "\n",
    "def find_best_pair(freq):\n",
    "    \"\"\"\n",
    "    找到频次最高的字符对。\n",
    "    \n",
    "    参数：\n",
    "    \tfreq: 字符对频次的字典\n",
    "    \t\n",
    "    返回：\n",
    "    \t频次最高的字符对及其频次\n",
    "    \"\"\"\n",
    "    if not freq:\n",
    "        return None, 0\n",
    "    best_pair = max(freq, key=freq.get)\n",
    "    return best_pair, freq[best_pair]\n",
    "\n",
    "def merge_pair(word_freq, pair_to_merge):\n",
    "    \"\"\"\n",
    "    合并指定的字符对到新符号。\n",
    "    \n",
    "    参数：\n",
    "        word_freq: List of tuples, 每个元组包含单词（列表形式）和其频次\n",
    "    \tpair_to_merge: 要合并的字符对\n",
    "    \n",
    "    返回：\n",
    "    \t更新后的单词频次列表\n",
    "    \"\"\"\n",
    "    merged_word_freq = []\n",
    "    pair_str = ''.join(pair_to_merge)\n",
    "    for word, freq in word_freq:\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            # 检查当前字符和下一个字符是否是要合并的字符对\n",
    "            if i < len(word) - 1 and word[i] == pair_to_merge[0] and word[i + 1] == pair_to_merge[1]:\n",
    "                new_word.append(pair_str)\n",
    "                i += 2  # 跳过下一个字符，因为已合并\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        merged_word_freq.append((new_word, freq))\n",
    "    return merged_word_freq\n",
    "\n",
    "def bpe_merge(word_freq, vocab_size):\n",
    "    \"\"\"\n",
    "    执行 BPE 合并操作，直到词汇表达到预定大小。\n",
    "    \n",
    "    参数：\n",
    "\t\tword_freq: List of tuples, 每个元组包含单词（列表形式）和其频次\n",
    "\t\tvocab_size: 预定的词汇表大小\n",
    "    \n",
    "    返回：\n",
    "    \t最终词汇表和合并记录\n",
    "    \"\"\"\n",
    "    # 初始化词汇表\n",
    "    vocab = set()\n",
    "    for word, _ in word_freq:\n",
    "        vocab.update(word)\n",
    "    merges = []\n",
    "    \n",
    "    while len(vocab) < vocab_size:\n",
    "        pair_freq = count_char_pairs(word_freq)\n",
    "        best_pair, best_freq = find_best_pair(pair_freq)\n",
    "        if not best_pair:\n",
    "            break\n",
    "        # 合并最佳字符对\n",
    "        word_freq = merge_pair(word_freq, best_pair)\n",
    "        new_symbol = ''.join(best_pair)\n",
    "        vocab.add(new_symbol)\n",
    "        merges.append((best_pair, new_symbol))\n",
    "        print(f\"Merge: {best_pair} -> {new_symbol}, 词汇表大小: {len(vocab)}\")\n",
    "            \n",
    "    return vocab, merges\n",
    "\n",
    "# 示例\n",
    "word_freq = [\n",
    "    (['l', 'o', 'w'], 5),\n",
    "    (['l', 'o', 'w', 'e', 'r'], 2),\n",
    "    (['n', 'e', 'w', 'e', 's', 't'], 6),\n",
    "    (['w', 'i', 'd', 'e', 's', 't'], 3)\n",
    "]\n",
    "\n",
    "# 预定词汇表大小为13\n",
    "final_vocab, merge_records = bpe_merge(word_freq, 13)\n",
    "\n",
    "print(\"\\n最终词汇表 V:\")\n",
    "print(final_vocab)\n",
    "\n",
    "print(\"\\n合并记录:\")\n",
    "for idx, (pair, new_sym) in enumerate(merge_records, 1):\n",
    "    print(f\"Merge {idx}: {pair} -> {new_sym}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b3de578-f6cf-4bcd-b7e0-45d2ea6846df",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '（' (U+FF08) (2113497978.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    **Q2. 如果以`</w>`（end-of-word）作为每个语料库中单词的结尾，最初的词汇表会受到什么影响，后续的过程呢？假设预定大小为 14，当前的合并记录是什么？**\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '（' (U+FF08)\n"
     ]
    }
   ],
   "source": [
    "**Q2. 如果以`</w>`（end-of-word）作为每个语料库中单词的结尾，最初的词汇表会受到什么影响，后续的过程呢？假设预定大小为 14，当前的合并记录是什么？**\n",
    "\n",
    "- **初始词汇表 $V$**：\n",
    "\n",
    "  添加 `</w>` 后，词汇表变为：\n",
    "\n",
    "  ```plaintext\n",
    "  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', '</w>'}\n",
    "  ```\n",
    "\n",
    "  大小为 11。\n",
    "\n",
    "- **影响**：\n",
    "\n",
    "  合并过程和合并记录将会发生变化，因为 `</w>` 的存在会影响字符对的频次统计和合并顺序。\n",
    "\n",
    "- **合并记录**：\n",
    "\n",
    "  1. 合并 `(\"e\", \"s\")` -> `es`，词汇表大小增加到 12。\n",
    "  2. 合并 `(\"es\", \"t\")` -> `est`，词汇表大小增加到 13。\n",
    "  3. 合并 `(\"est\", \"<\\w>\")` -> `est<\\w>`，词汇表大小增加到 14。\n",
    "\n",
    "运行代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc46f02a-2533-423b-aa70-68d4f668a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge: ('e', 's') -> es, 词汇表大小: 12\n",
      "Merge: ('es', 't') -> est, 词汇表大小: 13\n",
      "Merge: ('est', '</w>') -> est</w>, 词汇表大小: 14\n",
      "\n",
      "最终词汇表 V:\n",
      "{'es', 'n', 'r', 'e', 'est', 'w', 'i', 'o', 'est</w>', 'l', 's', 't', 'd', '</w>'}\n",
      "\n",
      "合并记录:\n",
      "Merge 1: ('e', 's') -> es\n",
      "Merge 2: ('es', 't') -> est\n",
      "Merge 3: ('est', '</w>') -> est</w>\n"
     ]
    }
   ],
   "source": [
    "# 示例\n",
    "word_freq = [\n",
    "    (['l', 'o', 'w', '</w>'], 5),\n",
    "    (['l', 'o', 'w', 'e', 'r', '</w>'], 2),\n",
    "    (['n', 'e', 'w', 'e', 's', 't', '</w>'], 6),\n",
    "    (['w', 'i', 'd', 'e', 's', 't', '</w>'], 3)\n",
    "]\n",
    "\n",
    "# 预定词汇表大小为14\n",
    "final_vocab, merge_records = bpe_merge(word_freq, 14)\n",
    "\n",
    "print(\"\\n最终词汇表 V:\")\n",
    "print(final_vocab)\n",
    "\n",
    "print(\"\\n合并记录:\")\n",
    "for idx, (pair, new_sym) in enumerate(merge_records, 1):\n",
    "    print(f\"Merge {idx}: {pair} -> {new_sym}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a88f0-86c3-4625-9b7d-c9f5e7c10909",
   "metadata": {},
   "source": [
    "### 标记文本\n",
    "\n",
    "你可能已经注意到，每次合并时我们都会记录对应的 **merge** 规则，但并未详细说明其作用，下面将以 BPE 为例进行解释。\n",
    "\n",
    "#### BPE\n",
    "\n",
    "在之前的示例中，三轮合并后将得到以下合并规则（按合并顺序排列）：  \n",
    "\n",
    "1. 合并字符对 `'e'` 和 `'s'`，得到 `'es'`。\n",
    "2. 合并字符对 `'es'` 和 `'t'`，得到 `'est'`。\n",
    "3. 合并字符对 `'l'` 和 `'o'`，得到 `'lo'`。\n",
    "\n",
    "假设当前词汇表包含所有单个字符，修改[官方文档](https://huggingface.co/learn/nlp-course/en/chapter6/5)最后提供的 tokenize() 示例代码进行演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9511ff0-6d34-442e-80cd-b1119b896eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始预分词结果:\n",
      "['estimate', ',', 'Ġlocal']\n",
      "\n",
      "初始拆分结果:\n",
      "[['e', 's', 't', 'i', 'm', 'a', 't', 'e'], [','], ['Ġ', 'l', 'o', 'c', 'a', 'l']]\n",
      "\n",
      "应用合并规则: ('e', 's') -> es\n",
      "  合并前第 1 个单词: ['e', 's', 't', 'i', 'm', 'a', 't', 'e']\n",
      "    在位置 0 处合并: ['es', 't', 'i', 'm', 'a', 't', 'e']\n",
      "  合并前第 2 个单词: [',']\n",
      "  合并前第 3 个单词: ['Ġ', 'l', 'o', 'c', 'a', 'l']\n",
      "\n",
      "应用合并规则: ('es', 't') -> est\n",
      "  合并前第 1 个单词: ['es', 't', 'i', 'm', 'a', 't', 'e']\n",
      "    在位置 0 处合并: ['est', 'i', 'm', 'a', 't', 'e']\n",
      "  合并前第 2 个单词: [',']\n",
      "  合并前第 3 个单词: ['Ġ', 'l', 'o', 'c', 'a', 'l']\n",
      "\n",
      "应用合并规则: ('l', 'o') -> lo\n",
      "  合并前第 1 个单词: ['est', 'i', 'm', 'a', 't', 'e']\n",
      "  合并前第 2 个单词: [',']\n",
      "  合并前第 3 个单词: ['Ġ', 'l', 'o', 'c', 'a', 'l']\n",
      "    在位置 1 处合并: ['Ġ', 'lo', 'c', 'a', 'l']\n",
      "\n",
      "最终拆分结果:\n",
      "[['est', 'i', 'm', 'a', 't', 'e'], [','], ['Ġ', 'lo', 'c', 'a', 'l']]\n",
      "\n",
      "最终生成的 Tokens:\n",
      "['est', 'i', 'm', 'a', 't', 'e', ',', 'Ġ', 'lo', 'c', 'a', 'l']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    # 预分词处理：将文本拆分为初步的单词列表\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "\n",
    "    print(\"初始预分词结果:\")\n",
    "    print(pre_tokenized_text)\n",
    "\n",
    "    # 将每个单词拆分为字符列表\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    print(\"\\n初始拆分结果:\")\n",
    "    print(splits)\n",
    "\n",
    "    # 遍历所有合并规则（merges），逐步应用到拆分后的结果中\n",
    "    for pair, merge in merges.items():\n",
    "        print(f\"\\n应用合并规则: {pair} -> {merge}\")\n",
    "\n",
    "        # 遍历每个已拆分的单词\n",
    "        for idx, split in enumerate(splits):\n",
    "            print(f\"  合并前第 {idx+1} 个单词: {split}\")\n",
    "            i = 0\n",
    "            # 在当前拆分的字符中查找匹配的字符对\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    # 合并字符对\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    print(f\"    在位置 {i} 处合并: {split}\")\n",
    "                else:\n",
    "                    i += 1\n",
    "            # 更新拆分后的结果\n",
    "            splits[idx] = split\n",
    "\n",
    "    print(\"\\n最终拆分结果:\")\n",
    "    print(splits)\n",
    "\n",
    "    # 将所有拆分后的结果合并为一个 Token 列表并返回\n",
    "    return sum(splits, [])\n",
    "\n",
    "# 示例 merges 字典\n",
    "merges = {\n",
    "    ('e', 's'): 'es',\n",
    "    ('es', 't'): 'est',\n",
    "    ('l', 'o'): 'lo'\n",
    "}\n",
    "\n",
    "# 示例文本\n",
    "text = \"estimate, local\"\n",
    "\n",
    "# 调用 tokenize 函数，并打印中间过程\n",
    "tokens = tokenize(text)\n",
    "print(\"\\n最终生成的 Tokens:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951e873-4468-4a40-b959-f8ee645a4ec9",
   "metadata": {},
   "source": [
    "不过，在之前的过程中生成的最终词汇表 $V$ 并未包含所有单个字符，而是：  \n",
    "\n",
    "```\n",
    "{'e', 'r', 's', 'est', 'w', 'l', 'o', 'lo', 'es', 'i', 'n', 't', 'd'}\n",
    "```\n",
    "\n",
    "因此，对于输入 `\"estimate, local\"`，其标记结果为：  \n",
    "\n",
    "```\n",
    "['est', 'i', '[UNK]', 'a', 't', 'e', '[UNK]', 'lo', '[UNK]', '[UNK]', l]\n",
    "```\n",
    "\n",
    "这里的 `'[UNK]'`（UNKNOWN）表示该子词不在词汇表中，即属于 **OOV（Out-of-Vocabulary）** 的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85de72-6c72-414e-a2ef-574f3f745350",
   "metadata": {},
   "source": [
    "#### WordPiece\n",
    "\n",
    "和 BPE 不同，WordPiece 对 OOV 采取的是「宁杀错不放过」策略，即只要有一个字符没见过，整个单词都标记为 `'[UNK]'`。\n",
    "\n",
    "修改[官方文档](https://huggingface.co/learn/nlp-course/en/chapter6/6)最后提供的 tokenize() 示例代码进行演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59dd5189-0e5c-430b-90bf-8794cdcd8410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "初始预分词结果:\n",
      "['estimate', ',', 'local', ',', 'lows']\n",
      "\n",
      "正在标记单词: estimate\n",
      "  [UNK] 标记: estimate\n",
      "  标记结果: ['[UNK]']\n",
      "\n",
      "正在标记单词: ,\n",
      "  [UNK] 标记: ,\n",
      "  标记结果: ['[UNK]']\n",
      "\n",
      "正在标记单词: local\n",
      "  匹配到 Token: lo\n",
      "  剩余部分添加前缀: ##cal\n",
      "  [UNK] 标记: ##cal\n",
      "  标记结果: ['[UNK]']\n",
      "\n",
      "正在标记单词: ,\n",
      "  [UNK] 标记: ,\n",
      "  标记结果: ['[UNK]']\n",
      "\n",
      "正在标记单词: lows\n",
      "  匹配到 Token: lo\n",
      "  剩余部分添加前缀: ##ws\n",
      "  匹配到 Token: ##w\n",
      "  剩余部分添加前缀: ##s\n",
      "  匹配到 Token: ##s\n",
      "  标记结果: ['lo', '##w', '##s']\n",
      "\n",
      "最终标记结果:\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', 'lo', '##w', '##s']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize(text):\n",
    "    # 预分词处理：将文本拆分为初步的单词列表\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "\n",
    "    print(\"\\n初始预分词结果:\")\n",
    "    print(pre_tokenized_text)\n",
    "\n",
    "    # 对每个单词进行标记\n",
    "    tokenized_words = []\n",
    "    for word in pre_tokenized_text:\n",
    "        tokens = []\n",
    "        print(f\"\\n正在标记单词: {word}\")\n",
    "        \n",
    "        while len(word) > 0:\n",
    "            i = len(word)\n",
    "            # 尝试匹配词汇表中的最长子词\n",
    "            while i > 0 and word[:i] not in vocab:\n",
    "                i -= 1\n",
    "            if i == 0:\n",
    "                print(f\"  [UNK] 标记: {word}\")\n",
    "                tokens = [\"[UNK]\"]  # 没有匹配到则返回 [UNK]\n",
    "                break  # 跳出循环，不再继续处理该单词\n",
    "\n",
    "            # 匹配到子词，添加到 tokens 列表中\n",
    "            matched_token = word[:i]\n",
    "            tokens.append(matched_token)\n",
    "            print(f\"  匹配到 Token: {matched_token}\")\n",
    "\n",
    "            # 更新剩余部分，并添加“##”作为前缀\n",
    "            word = word[i:]\n",
    "            if len(word) > 0:\n",
    "                word = f\"##{word}\"\n",
    "                print(f\"  剩余部分添加前缀: {word}\")\n",
    "\n",
    "        print(f\"  标记结果: {tokens}\")\n",
    "        tokenized_words.append(tokens)\n",
    "\n",
    "    print(\"\\n最终标记结果:\")\n",
    "    flattened_tokens = sum(tokenized_words, [])  # 展平成单层列表\n",
    "    print(flattened_tokens)\n",
    "\n",
    "    return flattened_tokens\n",
    "\n",
    "# 示例词汇表\n",
    "vocab = {'##st', 'n', '##i', '##s', 'wid', '##d', 'wi', '##r', '##o', \n",
    "         'lo', 'w', '##e', '##w', '##t', 'l'}\n",
    "\n",
    "# 示例文本\n",
    "text = \"estimate, local, lows\"\n",
    "\n",
    "# 使用 BERT 的分词器（WordPiece）\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 调用 tokenize 函数，并打印中间过程\n",
    "tokens = tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f0d6b-fccd-45ed-92c2-127512850764",
   "metadata": {},
   "source": [
    "在 Transformers 中，**分词（tokenization）** 实际上包含以下几个步骤：  \n",
    "\n",
    "1. **标准化（Normalization）**：对文本进行必要的清理操作，例如删除多余空格或重音符号、进行 Unicode 标准化等。\n",
    "2. **预分词（Pre-tokenization）**：将输入拆分为单词。\n",
    "3. **通过模型处理输入（Running the input through the model）**：使用预分词后的单词生成一系列词元（tokens）。\n",
    "4. **后处理（Post-processing）**：添加分词器的特殊标记，生成注意力掩码（attention mask）和词元类型 ID（token type IDs）。\n",
    "\n",
    "[官方文档](https://huggingface.co/learn/nlp-course/en/chapter6/8)给出了一张整体流程图：\n",
    "\n",
    "![en_chapter6_tokenization_pipeline](../Guide/assets/en_chapter6_tokenization_pipeline.svg)\n",
    "\n",
    "运行代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0385adda-9eea-456e-99b3-a8cca01ad27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本: Hello how are U tday\n",
      "标准化后的文本: hello how are u tday\n",
      "预分词结果: [('hello', (0, 5)), ('how', (6, 9)), ('are', (10, 13)), ('u', (14, 15)), ('tday', (16, 20))]\n",
      "词元（Tokens）: ['hello', 'how', 'are', 'u', 'td', '##ay']\n",
      "词元 ID（Token IDs）: [7592, 2129, 2024, 1057, 14595, 4710]\n",
      "编码结果: {'input_ids': tensor([[  101,  7592,  2129,  2024,  1057, 14595,  4710,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "注意力掩码（Attention Mask）: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "词元类型 ID（Token Type IDs）: tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "解码后的文本: hello how are u tday\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载 BERT 的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 原始文本\n",
    "text = \"Hello how are U tday\"\n",
    "print(\"原始文本:\", text)\n",
    "\n",
    "# 1. 标准化：转换为小写\n",
    "normalized_text = text.lower()\n",
    "print(\"标准化后的文本:\", normalized_text)\n",
    "\n",
    "# 2. 预分词（Pre-tokenization）：将输入拆分为单词\n",
    "pre_tokenized = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(normalized_text)\n",
    "print(\"预分词结果:\", pre_tokenized)\n",
    "\n",
    "# 3. 分词：将预分词后的结果转换为子词级词元\n",
    "tokens = tokenizer.tokenize(normalized_text)\n",
    "print(\"词元（Tokens）:\", tokens)\n",
    "\n",
    "# 4. 将 tokens 转换为 token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"词元 ID（Token IDs）:\", token_ids)\n",
    "\n",
    "# 5. 编码（包含特殊标记和后处理）\n",
    "encoded = tokenizer(normalized_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"编码结果:\", encoded)\n",
    "\n",
    "# 6. 打印注意力掩码和词元类型 ID（后处理部分）\n",
    "print(\"注意力掩码（Attention Mask）:\", encoded[\"attention_mask\"])\n",
    "print(\"词元类型 ID（Token Type IDs）:\", encoded[\"token_type_ids\"])\n",
    "\n",
    "# 7. 解码：将 token IDs 转换回文本\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"解码后的文本:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32bb25-8169-4237-9216-05df067be214",
   "metadata": {},
   "source": [
    "## 映射（Mapping）\n",
    "\n",
    "以 BPE 为例，最终词汇表 $V$ 中的 Token 和对应的频次分别为：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28723d53-2500-4986-a363-70688c16ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'lo': 7,\n",
    "    'w': 16,\n",
    "    'e': 8,\n",
    "    'r': 2,\n",
    "    'n': 6,\n",
    "    'est': 9,\n",
    "    'i': 3,\n",
    "    'd': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e716182-86fc-47d7-b9cc-6d06ff85d9aa",
   "metadata": {},
   "source": [
    "简单实现 Token 和 ID 之间的映射关系的代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff684d26-c327-4d87-8aad-a31ecf11f8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to ID: {'lo': 0, 'w': 1, 'e': 2, 'r': 3, 'n': 4, 'est': 5, 'i': 6, 'd': 7}\n",
      "ID to Token: {0: 'lo', 1: 'w', 2: 'e', 3: 'r', 4: 'n', 5: 'est', 6: 'i', 7: 'd'}\n"
     ]
    }
   ],
   "source": [
    "# 创建 token 到 ID 的映射\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "\n",
    "# 创建 ID 到 token 的映射\n",
    "id_to_token = {idx: token for token, idx in token_to_id.items()}\n",
    "\n",
    "# 打印映射关系\n",
    "print(\"Token to ID:\", token_to_id)\n",
    "print(\"ID to Token:\", id_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903c7c9-c792-4520-ab64-6fe7dfd456c8",
   "metadata": {},
   "source": [
    "当然，也可以根据频次或者其他规则进行特殊处理。\n",
    "\n",
    "\n",
    "以上是编码部分的概述，实际上在文本预处理的时候还会增加特殊标记，但这些以及后续的解码部分大多是一些文本处理的规则，这里就不过多赘述了，Tokenizer 之间的核心差异在于使用的分割方法和词汇表的构建策略。\n",
    "\n",
    "## 参考链接\n",
    "\n",
    "- [Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/5)\n",
    "- [WordPiece tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebfbb6-fa74-4449-9acc-da34b7ef9f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
