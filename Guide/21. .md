> 蒙着头在应用上走了好几步，也该回头理解其中的原理了。以文本为例，今天我们来讲讲最开始处理数据的 Tokenzier。
>
> 文章的「构造词汇表」部分将介绍常见的两种分词方法：
>
> - BPE：GPT、GPT-2、RoBERTa、BART 和 DeBERTa 等。
> - WordPiece：DistilBERT、MobileBERT、Funnel Transformers 和 MPNET 等。
>
> 可视化：[Tiktokenizer](https://tiktokenizer.vercel.app)

## 什么是 Tokenizer？

**Tokenizer（分词器）** 可以将原始文本（raw text）转换为模型能够理解的数字序列，在两个主要阶段进行应用：模型输入和模型输出。

### 模型输入（编码 Encode）

1. **分词（Tokenize）**

   将文本拆分为词元（Token），常见的分词方式包括字级、词级、子词级（如 BPE、WordPiece）、空格分词等。

   ```sql
   输入: "你好"
   分词: ["你", "好"]
   ```

2. **映射（mapping）**

   每个词元被映射为词汇表（Vocabulary）中的唯一 ID，生成的数字序列即为模型的输入。  

   ```sql
   分词: ["你", "好"]
   映射: [1001, 1002]
   ```

### 模型输出（解码 Decode）

1. **反映射（De-mapping）**

   模型输出的数字序列需要通过词汇表映射回对应的词元。这一步是编码的逆过程，通过词汇表将 ID 映射回对应的词元，其实就是一一映射的关系。

   ```sql
   输出: [1001, 1002]
   反映射: ["你", "好"]
   ```

2. **文本重组**

   将解码后的词元拼接为完整的文本，需根据具体的语言和分词规则处理标点和空格。

   ```sql
   反映射: ["你", "好"]
   重组: "你好"
   ```

## 实际使用

在详细解释之前，先通过 Transformers 有个直观的感受。

### BPE

```python
from transformers import AutoTokenizer

# 使用 GPT-2 的分词器（BPE）
tokenizer = AutoTokenizer.from_pretrained("gpt2")

text = "Hello, world!"

# 编码
# 1. 将文本分词为 Tokens
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

# 2. 将 Tokens 转换为 Token IDs
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print("Token IDs:", token_ids)

# 解码
# 1. Token IDs 转换为 Tokens
tokens = tokenizer.convert_ids_to_tokens(token_ids)
print("Tokens:", tokens)

# 2. Tokens 拼接为文本
decoded_text = tokenizer.convert_tokens_to_string(tokens)
print("Decoded Text:", decoded_text)
```

输出：

```python
Tokens: ['Hello', ',', 'Ġworld', '!']
Token IDs: [15496, 11, 995, 0]
Tokens: ['Hello', ',', 'Ġworld', '!']
Decoded Text: Hello, world!
```



### WordPiece

```python
from transformers import AutoTokenizer

# 使用 BERT 的分词器（WordPiece）
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "Hello, world!"

# 编码
# 1. 将文本分词为 Tokens
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

# 2. 将 Tokens 转换为 Token IDs
token_ids = tokenizer.convert_tokens_to_ids(tokens)
print("Token IDs:", token_ids)

# 解码
# 1. Token IDs 转换为 Tokens
tokens = tokenizer.convert_ids_to_tokens(token_ids)
print("Tokens:", tokens)

# 2. Tokens 拼接为文本
decoded_text = tokenizer.convert_tokens_to_string(tokens)
print("Decoded Text:", decoded_text)

```

输出：

```python
Tokens: ['hello', ',', 'world', '!']
Token IDs: [7592, 1010, 2088, 999]
Tokens: ['hello', ',', 'world', '!']
Decoded Text: hello, world!
```



当然，更常见的使用方式是直接使用 `encode()` 和 `decode()` 方法，这里演示一下：

```python
from transformers import AutoTokenizer

# 取消对比一下二者输出上的差异
tokenizer = AutoTokenizer.from_pretrained("gpt2")
# tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "Hello, world!"

# 使用 encode() 将文本直接转换为 Token IDs
token_ids = tokenizer.encode(text)
print("Token IDs:", token_ids)

# 使用 decode() 将 Token IDs 转换回文本
decoded_text = tokenizer.decode(token_ids)
print("Decoded Text:", decoded_text)
```

输出：

```sql
Token IDs: [15496, 11, 995, 0]
Decoded Text: Hello, world!
```





下面将逐步展开进行讲解：


## 分词（Tokenize）

假设当前语料库（corpus）包含的单词和对应频次如下：

```sql
("low", 5), ("lower", 2), ("newest", 6), ("widest", 6)
```

有些论文也用 `vocab` 来表述，知道后面是频次即可，命名不用纠结。

### 构造词汇表

#### Byte-Pair Encoding (BPE)

> [A new algorithm for data compression. 1994](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)
>
> BPE 是一种基于数据压缩的技术，最早由 Gage 在 1994 年提出。
>
> 被用于 GPT，从字符级别开始，通过迭代合并频率最高的字符对（或字符序列）来构建新的 token，这样可以处理 OOV（Out-Of-Vocabulary）情况。
>
> **Q: 什么是 OOV ？**
>
> 其实就是不在词汇表中，又称之为「未登录词」。

#### 具体步骤

1. **初始化词汇表 $V$**：

   - $V$ 包含语料库中的所有唯一字符。

2. **统计字符对的频次**：

   - 对于每个单词的字符序列，统计相邻字符对的出现频次。

3. **找到频次最高的字符对并合并**：

   - 选择出现频率最高的字符对 $(x, y)$，将其合并为新符号 $xy$。

   - **Score 函数**：

     $$
     \text{Score}_{\text{BPE}}(x, y) = \text{freq}(x, y)
     $$

     其中，$\text{freq}(x, y)$ 表示字符对 $(x, y)$ 在语料库中的出现频次。

4. **更新词汇表并重复步骤 2 和 3**：

   - 将新符号添加到词汇表 $V = V \cup \{xy\}$，更新语料库中的单词表示，重复统计和合并过程，直到满足停止条件（例如，词汇表达到预定大小）。

#### 示例

**步骤 1：初始化词汇表**

- **将单词拆分为字符序列**：

  ```plaintext
  ("l", "o", "w"), 5  
  ("l", "o", "w", "e", "r"), 2  
  ("n", "e", "w", "e", "s", "t"), 6  
  ("w", "i", "d", "e", "s", "t"), 6
  ```

- **词汇表 $V$**：

  ```plaintext
  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd'}
  ```

**步骤 2：统计字符对的频次**

- **统计结果**：

  ```plaintext
  ("l", "o"): 7      (5 + 2)
  ("o", "w"): 7      (5 + 2)
  ("w", "e"): 8      (2 + 6)
  ("e", "r"): 2
  ("n", "e"): 6
  ("e", "w"): 6
  ("e", "s"): 12     (6 + 6)
  ("s", "t"): 12     (6 + 6)
  ("w", "i"): 6
  ("i", "d"): 6
  ("d", "e"): 6
  ```

**步骤 3：找到频次最高的字符对并合并**

- **选择频次最高的字符对**：
  - `("e", "s")` 和 `("s", "t")`，频次均为 12。可以任选其一进行合并，假设选择排序后第一的： `("e", "s")`。
- **合并 `("e", "s")` 为新符号 `es`**。

**步骤 4：更新词汇表并重复**

- **更新单词序列**：

  ```plaintext
  ("n", "e", "w", "es", "t"), 6  
  ("w", "i", "d", "es", "t"), 6
  ```

- **更新词汇表 $V$**：

  ```plaintext
  {'l', 'o', 'w', 'e', 'r', 'n', 's', 't', 'i', 'd', 'es'}
  ```

- **重复步骤 2，3，4，直到达到预定的词汇表大小**。





