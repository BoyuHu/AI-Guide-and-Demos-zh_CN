> 未完待续，攥写✍️中.. 
>
> 来用点主流的大型语言模型（LLM）。
>
> 前文《17. 浅谈模型量化：非对称 vs 对称》中我们有提到用 PyTorch 去做一些模型量化，但实际应用时，不需要这么麻烦，你知道的，我们可以使用 transformer 库中的轮子来完成这些操作。
>
> 你可能会产生一个疑惑：在之前不是有一篇唐诗微调 LLM 的文章吗？为什么还要再写一篇。
>
> 因为之前的 LLM 微调代码本质目的是带你去“用”，而非“写”，而这篇文章会从头开始进行本地的部署，你将真正了解到其中的方方面面。
>

访问 [Hugging Face](https://huggingface.co/models)，让我们先选择一个7B左右的语言模型：

1. [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)
2. [Qwen/Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
3. [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)

> [!NOTE]
>
> 你可以随意更换你喜欢的模型，上面只是简单列举出。

停一下，还记得 FP32 下 7B 模型的参数有多大吗？

“不严谨的说，好像是 28 GB，所以我们要用模型量化来导入模型，就是太大了，可能要下载比较久的时间:(”

是的，这个说法没有问题，不过上面列出的模型采用的是 BF16，所以还会更小点。

“那大概 14 GB，我晚点再开始正式学习，还是要下载很久”

诶，那你有没有想过，既然这些模型下载下来需要量化，为什么不直接去下一个量化版的模型？

是的，以上所列的三个模型，在 Hugging Face 中都有着量化版本：

1. [bartowski/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF)
2. [Qwen/Qwen2.5-7B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF)
3. [bartowski/Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF)



> ![NOTE]
>
> 1. 不要因为 ChatGPT 的存在将 GPT 的概念直接映射为 OpenAI，GPT（Generative Pre-trained Transformer）指的是**生成式预训练 Transformer**。
> 2. 如果你选择的是其他模型，用下面的方式去搜索是否存在量化版本，假设你要找的是 INT4：
>
>    - [模型名称]-[AWQ]/[GPTQ]/[GGUF]
>
>    - [模型名称]-[INT4]





