> 未完待续，攥写✍️中.. （当前commit会根据参考文献修正表述直到下一次的提交）
>
> 来用点主流的大型语言模型（LLM）。
>
> 前文《17. 浅谈模型量化：非对称 vs 对称》中我们有提到用 PyTorch 去做一些模型量化，但实际应用时，不需要这么麻烦，你知道的，我们可以使用 transformer 库中的轮子来完成这些操作。
>
> 你可能会产生一个疑惑：在之前不是有一篇唐诗微调 LLM 的文章吗？为什么还要再写一篇。
>
> 因为之前的 LLM 微调代码本质目的是带你去“用”，而非“写”，而这篇文章会从头开始进行本地的部署，你将真正了解到其中的方方面面。
>

访问 [Hugging Face](https://huggingface.co/models)，让我们先选择一个7B左右的语言模型：

1. [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)
2. [Qwen/Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
3. [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)

> [!NOTE]
>
> 你可以随意更换你喜欢的模型，上面只是简单列举出。

停一下，还记得 FP32 下 7B 模型的参数有多大吗？

“不严谨的说，好像是 28 GB，所以我们要用模型量化来导入模型，就是太大了，可能要下载比较久的时间:(”

是的，这个说法没有问题，不过上面列出的模型采用的是 BF16，所以还会更小点。

“那大概 14 GB，我晚点再开始正式学习，还是要下载很久”

诶，那你有没有想过，既然这些模型下载下来需要量化，为什么不直接去下一个量化版的模型？

是的，以上所列的三个模型，在 Hugging Face 中都有着量化版本：

1. [bartowski/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF)
2. [Qwen/Qwen2.5-7B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF)
3. [bartowski/Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF)

## GPTQ vs AWQ vs GGUF（GGML） 速览

> 参考链接：[GPTQ - 2210.17323](https://arxiv.org/pdf/2210.17323) | [AWQ - 2306.00978](https://arxiv.org/pdf/2306.00978) | [GGML](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#historical-state-of-affairs) | [GGUF - docs](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) | [What is GGUF and GGML?](https://medium.com/@phillipgimmi/what-is-gguf-and-ggml-e364834d241c)

- **GPTQ** (Generalized **Post-Training** Quantization)
  
  GPTQ 是一种基于近似二阶信息的**后训练**量化技术，能够将模型的权重位宽降低到 3-4 bits，在大幅减少模型大小和计算成本的同时还能保持模型性能。在极端情况下还能量化到 2 bits 甚至 3 进制，但会有一定的性能损失。
  
- **AWQ** (**Activation-aware** Weight Quantization)

  ![image-20241004182540373](./assets/20241004200458.png)

  AWQ 不会量化模型的所有权重，保留了对模型性能重要的一小部分权重，大大减少了量化损失。如图所示，这里比较极端，是 INT3 量化：

  - 图 a：**RTN量化（Round-to-Nearest）**
    
    将权重直接四舍五入到目标位宽，导致性能明显下降，PPL 达到 43.2。
    
  - 图 b：**保护 1% 的显著权重，使用混合精度形式**
    
    这里展示了一种改进策略，即保留 1% 最重要的权重通道使用高精度（FP16），其余使用低精度（INT3）。PPL 降低到 13.0。虽然这种方法能保住性能，但由于需要不同精度切换，硬件效率不高。但这一策略证明了并非所有权重都对模型性能同等重要。
    
  - 图 c：**AWQ 提出的通道缩放量化方法**

    AWQ 通过**通道缩放**保护显著权重，利用激活分布找到重要的权重并缩放它们的值来减少量化误差。相比混合精度形式，AWQ 提升了硬件效率，同时性能与图 b 一致，PPL 也为到 13.0。

- **GGML** (GPT-Generated Model Language)
  
  「显存不够内存来凑」，这是一种文件格式，支持在 CPU 和 GPU 上进行推理。
  
- **GGUF** (GPT-Generated Unified Format)：
  
  GGUF 是 GGML 的升级版，提升了扩展和兼容性。

> [!NOTE]
>
> #### 什么是 PPL？
>
> > 以下叙述参考自 [Hugging Face - docs](https://huggingface.co/docs/transformers/perplexity#perplexity-of-fixed-length-models)。
>
> PPL 是 **Perplexity（困惑度）** 的缩写，它是衡量语言模型性能的常用指标，特别是在评估**经典语言模型**（**自回归**或**因果语言模型**）的生成任务时。
>
> **Perplexity** 主要用于衡量一个模型对给定文本的预测能力。具体来说，PPL 反映了模型在处理一段文本时的“不确定性”或“困惑度”，数值越低，表示模型越擅长预测下一步的词汇。PPL 定义为序列的指数化平均负对数似然。如果我们有一个**标记化序列** $X = (x_0, x_1, \dots, x_t)$，那么该序列的困惑度定义为：
>
> $\text{PPL}(X) = \exp \left( -\frac{1}{t} \sum_{i=1}^{t} \log p_\theta (x_i \mid x_{<i}) \right)$
>
> 其中, $\log p_\theta (x_i \mid x_{<i})$ 是模型在给定前 $i-1$ 个词 $x_{<i}$ 的条件下，对第 $i$ 个词 $x_i$ 的对数似然。直观地，PPL 可以被看作是模型在语料库中指定词汇集合上进行均匀预测的能力评估。
>
> - **较低的 PPL**：表示模型更准确地预测了文本中的词汇，模型对语言结构的掌握更好。
> - **较高的 PPL**：表示模型对文本的预测较为不确定，困惑度高，模型的表现较差。
>
> 在 AWQ 和 GPTQ 的量化研究中，PPL 被用来评估模型在不同量化位宽下的性能表现，例如 3-bit 或 4-bit 量化对模型预测能力的影响。
>
> **计算固定长度模型的 PPL**
>
> 如果不受模型上下文长度的限制，我们可以通过自回归地分解序列，并在每一步根据整个前序子序列来评估模型的困惑度，如下所示：
>
> ![Full decomposition of a sequence with unlimited context length](./assets/ppl_full.gif)
>
> 但是在实际中，模型的上下文长度是有限的，例如 [GPT-2](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2) 的最大长度为 1024 个标记。因此，当 $t$ 超过 1024 时，我们无法直接计算 $p_\theta(x_t | x_{<t})$。
>
> 通常，序列会被分割为最大输入大小的子序列。如果最大输入大小为 $k$，我们就基于前 $k-1$ 个标记来近似计算 $x_t$ 的似然。另一种近似方法是将序列分割为不重叠的块，独立累加每个段的分解对数似然：
>
> ![Suboptimal PPL not taking advantage of full available context](./assets/ppl_chunked.gif)
>
> 这种方法计算速度快，但从图示可以看出存在的问题，在新的一个窗口开始时，上下文是有限的，这会导致更高的 PPL。所以很自然的想到**滑动窗口策略**。
>
> ![Sliding window PPL taking advantage of all available context](./assets/ppl_sliding.gif)
>
> 这种策略更接近真实的序列概率分解，并且通常会得到更低的 PPL。缺点是它需要为语料库中的每个标记进行单独的前向传播。一个折衷方案是采用**跨步滑动窗口**，即通过更大的步幅（Stride）移动上下文，而不是每次滑动一个标记（Token）。
>
> （🤔考虑是否缩减这部分的表述后续单开一章代码进行讲解，因为总感觉不够直观，即便有着hugging face文档链接）

### GGUF 文件命名

> 参考链接：[GGUF - docs](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) 

GGUF 格式将加载模型所需的所有信息封装在一个文件中，简化了模型的分发和部署。同时，GGUF 文件命名遵循 `<BaseName><SizeLabel><FineTune><Version><Encoding><Type><Shard>.gguf` 的规则，方便人们快速识别模型的关键信息。具体说明如下：

- **BaseName**：模型的基础名称或架构名称，例如 `Llama`。

- **SizeLabel**：模型的参数规模标签，表示模型的参数数量及可能的专家数量，格式为 `<expertCount>x<count><scale-prefix>`。

  - **expertCount**：表示专家模型中的专家数量。如果模型没有使用 Mixture of Experts (MoE) 架构，可以省略。

  - **Count**：

    - `Q`: 表示百万亿（quadrillion）参数。
    - `T`: 表示万亿（trillion）参数。
    - `B`: 表示十亿（billion）参数。
    - `M`: 表示百万（million）参数。
    - `K`: 表示千（thousand）参数。

    当前主流大模型多为 B 级参数（十亿级），但未来 T（万亿级）模型可能会成为主流。

    更详细的内容见[附录](#附录)部分。

  - **附加属性**：在某些情况下，`-<attributes><count><scale-prefix>` 可以进一步细化模型的描述，添加额外的参数，例如 `Q`, `K`, `T`，这些表示量化方式或其他模型特性。例如：

    - `Q4`: 表示 4-bit 量化。

    示例：

    - `7B`: 表示 70 亿参数的模型。
    - `4x3T`: 表示有 4 个专家的 3 万亿参数模型。
    - `2x10B-Q4`: 表示有 2 个专家且采用 Q4 量化的 100 亿参数模型。

- **FineTune**：微调目标描述（如 `Chat`、`Instruct`）。

- **Version**（可选）：模型的版本号，格式为 `v<Major>.<Minor>`，没提供则假设为 `v1.0`。

- **Encoding**：权重编码方案（如 `Q4_0` 表示 4-bit 量化）。

- **Type**：文件类型，如 `LoRA`（适配器）或 `vocab`（仅包含词汇表）。

- **Shard**（可选）：模型分片信息，格式为 `<ShardNum>-of-<ShardTotal>`，适用于大型模型。例如 `00003-of-00009` 表示第 3 个分片，共 9 个分片，注意分片编号从 `00001` 开始，而非 `00000`。

> [!TIP]
>
> 验证命名是否符合规范的正则：
>
> ```
> ^(?<BaseName>[A-Za-z0-9\s]*(?:(?:-(?:(?:[A-Za-z\s][A-Za-z0-9\s]*)|(?:[0-9\s]*)))*))-(?:(?<SizeLabel>(?:\d+x)?(?:\d+\.)?\d+[A-Za-z](?:-[A-Za-z]+(\d+\.)?\d+[A-Za-z]+)?)(?:-(?<FineTune>[A-Za-z0-9\s-]+))?)?-(?:(?<Version>v\d+(?:\.\d+)*))(?:-(?<Encoding>(?!LoRA|vocab)[\w_]+))?(?:-(?<Type>LoRA|vocab))?(?:-(?<Shard>\d{5}-of-\d{5}))?\.gguf$
> ```

尝试理解下面三个来自官方文档的文件命名，看看你能否正确解析：

1. **Mixtral-8x7B-v0.1-KQ2.gguf**
2. **Hermes-2-Pro-Llama-3-8B-F16.gguf**
3. **Grok-100B-v1.0-Q4_0-00003-of-00009.gguf**

在文章的末尾会给出解析答案，现在请停下来思考。

### GGUF 文件结构

![*diagram by @mishig25(GGUF v3)*](./assets/313174776-c3623641-3a1d-408e-bfaf-1b7c4e16aa63-2.png)

如果想进一步了解，查看[附录](#附录)部分的代码。


> [!NOTE]
>
> 1. 不要因为 ChatGPT 的存在将 GPT 的概念直接映射为 OpenAI，GPT（Generative Pre-trained Transformer）指的是**生成式预训练 Transformer**。
> 2. 如果你选择的是其他模型，用下面的方式去搜索是否存在量化版本，假设你要找的是 INT4：
>
> - [模型名称]-[AWQ]/[GPTQ]/[GGUF]
> - [模型名称]-[INT4]

## 未完待续...







### 文件名解析答案

- **Mixtral-8x7B-v0.1-KQ2.gguf**：
  - **BaseName**：Mixtral
  - **SizeLabel**：
    - Expert Count: 8
    - Parameter Count: 7B
  - **Version**：v0.1
  - **Encoding**：KQ2

- **Hermes-2-Pro-Llama-3-8B-F16.gguf**：
  - **BaseName**：Hermes 2 Pro Llama 3
  - **SizeLabel**：
    - Expert Count: 0
    - Parameter Count: 8B
  - **Version**：v1.0
  - **Encoding**：F16

- **Grok-100B-v1.0-Q4_0-00003-of-00009.gguf**：
  - **BaseName**：Grok
  - **SizeLabel**：
    - Expert Count: 0
    - Parameter Count: 100B
  - **Version**：v1.0
  - **Encoding**：Q4_0
  - **Shard**：第 3 个分片，共 9 个分片

## 附录

### GGUF 文件命名

> [Quantization Types](https://huggingface.co/docs/hub/gguf#quantization-types)

| 类型    | 来源                                                         | 描述                                                         |
| ------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| F64     | [Wikipedia](https://en.wikipedia.org/wiki/Double-precision_floating-point_format) | 64 位标准 IEEE 754 双精度浮点数。                            |
| I64     | [GH](https://github.com/ggerganov/llama.cpp/pull/6062)       | 64 位定宽整数。                                              |
| F32     | [Wikipedia](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) | 32 位标准 IEEE 754 单精度浮点数。                            |
| I32     | [GH](https://github.com/ggerganov/llama.cpp/pull/6045)       | 32 位定宽整数。                                              |
| F16     | [Wikipedia](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) | 16 位标准 IEEE 754 半精度浮点数。                            |
| BF16    | [Wikipedia](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format) | 32 位 IEEE 754 单精度浮点数的 16 位简化版本。                |
| I16     | [GH](https://github.com/ggerganov/llama.cpp/pull/6045)       | 16 位定宽整数。                                              |
| Q8_0    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557654249) | 8 位四舍五入量化（q）。每个块有 32 个权重。权重公式：`w = q * block_scale`。目前已不广泛使用的过时量化方法。 |
| Q8_1    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557682290) | 8 位四舍五入量化（q）。每个块有 32 个权重。权重公式：`w = q * block_scale + block_minimum`。目前已不广泛使用的过时量化方法。 |
| Q8_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 8 位量化（q）。每个块有 256 个权重。仅用于量化中间结果。此量化类型支持所有 2-6 位点积。权重公式：`w = q * block_scale`。 |
| I8      | [GH](https://github.com/ggerganov/llama.cpp/pull/6045)       | 8 位定宽整数。                                               |
| Q6_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 6 位量化（q）。超块包含 16 个块，每个块有 16 个权重。权重公式：`w = q * block_scale(8-bit)`，每个权重占用 6.5625 位。 |
| Q5_0    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557654249) | 5 位四舍五入量化（q）。每个块有 32 个权重。权重公式：`w = q * block_scale`。目前已不广泛使用的过时量化方法。 |
| Q5_1    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557682290) | 5 位四舍五入量化（q）。每个块有 32 个权重。权重公式：`w = q * block_scale + block_minimum`。目前已不广泛使用的过时量化方法。 |
| Q5_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 5 位量化（q）。超块包含 8 个块，每个块有 32 个权重。权重公式：`w = q * block_scale(6-bit) + block_min(6-bit)`，每个权重占用 5.5 位。 |
| Q4_0    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557654249) | 4 位四舍五入量化（q）。每个块有 32 个权重。权重公式：`w = q * block_scale`。目前已不广泛使用的过时量化方法。 |
| Q4_1    | [GH](https://github.com/huggingface/huggingface.js/pull/615#discussion_r1557682290) | 4 位四舍五入量化（q）。每个块有 32 个权重。权重公式：`w = q * block_scale + block_minimum`。目前已不广泛使用的过时量化方法。 |
| Q4_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 4 位量化（q）。超块包含 8 个块，每个块有 32 个权重。权重公式：`w = q * block_scale(6-bit) + block_min(6-bit)`，每个权重占用 4.5 位。 |
| Q3_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 3 位量化（q）。超块包含 16 个块，每个块有 16 个权重。权重公式：`w = q * block_scale(6-bit)`，每个权重占用 3.4375 位。 |
| Q2_K    | [GH](https://github.com/ggerganov/llama.cpp/pull/1684#issue-1739619305) | 2 位量化（q）。超块包含 16 个块，每个块有 16 个权重。权重公式：`w = q * block_scale(4-bit) + block_min(4-bit)`，每个权重占用 2.5625 位。 |
| IQ4_NL  | [GH](https://github.com/ggerganov/llama.cpp/pull/5590)       | 4 位量化（q）。超块包含 256 个权重。权重 `w` 通过 `super_block_scale` 和 `importance matrix` 计算得到。 |
| IQ4_XS  | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 4 位量化（q）。超块包含 256 个权重。权重 `w` 通过 `super_block_scale` 和 `importance matrix` 计算得到，每个权重占用 4.25 位。 |
| IQ3_S   | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 3 位量化（q）。超块包含 256 个权重。权重 `w` 通过 `super_block_scale` 和 `importance matrix` 计算得到，每个权重占用 3.44 位。 |
| IQ3_XXS | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 3 位量化（q）。超块包含 256 个权重。权重 `w` 通过 `super_block_scale` 和 `importance matrix` 计算得到，每个权重占用 3.06 位。 |
| IQ2_XXS | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 2 位量化（q）。超块包含 256 个权重。权重 `w` 通过 `super_block_scale` 和 `importance matrix` 计算得到，每个权重占用 2.06 位。 |
| IQ2_S   | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 2 位量化（q）。超块包含 256 个权重。权重 `w` 通过 `super_block_scale` 和 `importance matrix` 计算得到，每个权重占用 2.5 位。 |
| IQ2_XS  | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 2 位量化（q）。超块包含 256 个权重。权重 `w` 通过 `super_block_scale` 和 `importance matrix` 计算得到，每个权重占用 2.31 位。 |
| IQ1_S   | [HF](https://huggingface.co/CISCai/OpenCodeInterpreter-DS-6.7B-SOTA-GGUF/blob/main/README.md?code=true#L59-L70) | 1 位量化（q）。超块包含 256 个权重。权重 `w` 通过 `super_block_scale` 和 `importance matrix` 计算得到，每个权重占用 1.56 位。 |
| IQ1_M   | [GH](https://github.com/ggerganov/llama.cpp/pull/6302)       | 1 位量化（q）。超块包含 256 个权重。权重 `w` 通过 `super_block_scale` 和 `importance matrix` 计算得到，每个权重占用 1.75 位。 |

### GGUF 文件结构

```c
enum ggml_type: uint32_t {
    GGML_TYPE_F32     = 0,
    GGML_TYPE_F16     = 1,
    GGML_TYPE_Q4_0    = 2,
    GGML_TYPE_Q4_1    = 3,
    // GGML_TYPE_Q4_2 = 4, support has been removed
    // GGML_TYPE_Q4_3 = 5, support has been removed
    GGML_TYPE_Q5_0    = 6,
    GGML_TYPE_Q5_1    = 7,
    GGML_TYPE_Q8_0    = 8,
    GGML_TYPE_Q8_1    = 9,
    GGML_TYPE_Q2_K    = 10,
    GGML_TYPE_Q3_K    = 11,
    GGML_TYPE_Q4_K    = 12,
    GGML_TYPE_Q5_K    = 13,
    GGML_TYPE_Q6_K    = 14,
    GGML_TYPE_Q8_K    = 15,
    GGML_TYPE_IQ2_XXS = 16,
    GGML_TYPE_IQ2_XS  = 17,
    GGML_TYPE_IQ3_XXS = 18,
    GGML_TYPE_IQ1_S   = 19,
    GGML_TYPE_IQ4_NL  = 20,
    GGML_TYPE_IQ3_S   = 21,
    GGML_TYPE_IQ2_S   = 22,
    GGML_TYPE_IQ4_XS  = 23,
    GGML_TYPE_I8      = 24,
    GGML_TYPE_I16     = 25,
    GGML_TYPE_I32     = 26,
    GGML_TYPE_I64     = 27,
    GGML_TYPE_F64     = 28,
    GGML_TYPE_IQ1_M   = 29,
    GGML_TYPE_COUNT,
};

enum gguf_metadata_value_type: uint32_t {
    // The value is a 8-bit unsigned integer.
    GGUF_METADATA_VALUE_TYPE_UINT8 = 0,
    // The value is a 8-bit signed integer.
    GGUF_METADATA_VALUE_TYPE_INT8 = 1,
    // The value is a 16-bit unsigned little-endian integer.
    GGUF_METADATA_VALUE_TYPE_UINT16 = 2,
    // The value is a 16-bit signed little-endian integer.
    GGUF_METADATA_VALUE_TYPE_INT16 = 3,
    // The value is a 32-bit unsigned little-endian integer.
    GGUF_METADATA_VALUE_TYPE_UINT32 = 4,
    // The value is a 32-bit signed little-endian integer.
    GGUF_METADATA_VALUE_TYPE_INT32 = 5,
    // The value is a 32-bit IEEE754 floating point number.
    GGUF_METADATA_VALUE_TYPE_FLOAT32 = 6,
    // The value is a boolean.
    // 1-byte value where 0 is false and 1 is true.
    // Anything else is invalid, and should be treated as either the model being invalid or the reader being buggy.
    GGUF_METADATA_VALUE_TYPE_BOOL = 7,
    // The value is a UTF-8 non-null-terminated string, with length prepended.
    GGUF_METADATA_VALUE_TYPE_STRING = 8,
    // The value is an array of other values, with the length and type prepended.
    ///
    // Arrays can be nested, and the length of the array is the number of elements in the array, not the number of bytes.
    GGUF_METADATA_VALUE_TYPE_ARRAY = 9,
    // The value is a 64-bit unsigned little-endian integer.
    GGUF_METADATA_VALUE_TYPE_UINT64 = 10,
    // The value is a 64-bit signed little-endian integer.
    GGUF_METADATA_VALUE_TYPE_INT64 = 11,
    // The value is a 64-bit IEEE754 floating point number.
    GGUF_METADATA_VALUE_TYPE_FLOAT64 = 12,
};

// A string in GGUF.
struct gguf_string_t {
    // The length of the string, in bytes.
    uint64_t len;
    // The string as a UTF-8 non-null-terminated string.
    char string[len];
};

union gguf_metadata_value_t {
    uint8_t uint8;
    int8_t int8;
    uint16_t uint16;
    int16_t int16;
    uint32_t uint32;
    int32_t int32;
    float float32;
    uint64_t uint64;
    int64_t int64;
    double float64;
    bool bool_;
    gguf_string_t string;
    struct {
        // Any value type is valid, including arrays.
        gguf_metadata_value_type type;
        // Number of elements, not bytes
        uint64_t len;
        // The array of values.
        gguf_metadata_value_t array[len];
    } array;
};

struct gguf_metadata_kv_t {
    // The key of the metadata. It is a standard GGUF string, with the following caveats:
    // - It must be a valid ASCII string.
    // - It must be a hierarchical key, where each segment is `lower_snake_case` and separated by a `.`.
    // - It must be at most 2^16-1/65535 bytes long.
    // Any keys that do not follow these rules are invalid.
    gguf_string_t key;

    // The type of the value.
    // Must be one of the `gguf_metadata_value_type` values.
    gguf_metadata_value_type value_type;
    // The value.
    gguf_metadata_value_t value;
};

struct gguf_header_t {
    // Magic number to announce that this is a GGUF file.
    // Must be `GGUF` at the byte level: `0x47` `0x47` `0x55` `0x46`.
    // Your executor might do little-endian byte order, so it might be
    // check for 0x46554747 and letting the endianness cancel out.
    // Consider being *very* explicit about the byte order here.
    uint32_t magic;
    // The version of the format implemented.
    // Must be `3` for version described in this spec, which introduces big-endian support.
    //
    // This version should only be increased for structural changes to the format.
    // Changes that do not affect the structure of the file should instead update the metadata
    // to signify the change.
    uint32_t version;
    // The number of tensors in the file.
    // This is explicit, instead of being included in the metadata, to ensure it is always present
    // for loading the tensors.
    uint64_t tensor_count;
    // The number of metadata key-value pairs.
    uint64_t metadata_kv_count;
    // The metadata key-value pairs.
    gguf_metadata_kv_t metadata_kv[metadata_kv_count];
};

uint64_t align_offset(uint64_t offset) {
    return offset + (ALIGNMENT - (offset % ALIGNMENT)) % ALIGNMENT;
}

struct gguf_tensor_info_t {
    // The name of the tensor. It is a standard GGUF string, with the caveat that
    // it must be at most 64 bytes long.
    gguf_string_t name;
    // The number of dimensions in the tensor.
    // Currently at most 4, but this may change in the future.
    uint32_t n_dimensions;
    // The dimensions of the tensor.
    uint64_t dimensions[n_dimensions];
    // The type of the tensor.
    ggml_type type;
    // The offset of the tensor's data in this file in bytes.
    //
    // This offset is relative to `tensor_data`, not to the start
    // of the file, to make it easier for writers to write the file.
    // Readers should consider exposing this offset relative to the
    // file to make it easier to read the data.
    //
    // Must be a multiple of `ALIGNMENT`. That is, `align_offset(offset) == offset`.
    uint64_t offset;
};

struct gguf_file_t {
    // The header of the file.
    gguf_header_t header;

    // Tensor infos, which can be used to locate the tensor data.
    gguf_tensor_info_t tensor_infos[header.tensor_count];

    // Padding to the nearest multiple of `ALIGNMENT`.
    //
    // That is, if `sizeof(header) + sizeof(tensor_infos)` is not a multiple of `ALIGNMENT`,
    // this padding is added to make it so.
    //
    // This can be calculated as `align_offset(position) - position`, where `position` is
    // the position of the end of `tensor_infos` (i.e. `sizeof(header) + sizeof(tensor_infos)`).
    uint8_t _padding[];

    // Tensor data.
    //
    // This is arbitrary binary data corresponding to the weights of the model. This data should be close
    // or identical to the data in the original model file, but may be different due to quantization or
    // other optimizations for inference. Any such deviations should be recorded in the metadata or as
    // part of the architecture definition.
    //
    // Each tensor's data must be stored within this array, and located through its `tensor_infos` entry.
    // The offset of each tensor's data must be a multiple of `ALIGNMENT`, and the space between tensors
    // should be padded to `ALIGNMENT` bytes.
    uint8_t tensor_data[];
};
```

